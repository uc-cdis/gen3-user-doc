{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Gen3 Documentation\n\uf0c1\n\n\n\n\n Overview \n\n\n\nThe Gen3 data commons allows approved researchers access to raw unprocessed datasets in a scalable, reproducible, privacy and security protected manner. This documentation describes the commons and provides a \nuser guide\n to assist contributors.\n\n\n Mission \n\n\n\nThe Gen3 commons was developed to accelerate scientific discovery through creation of a collaborative infrastructure that enables sharing of information between stakeholders in industry, academia, and regulatory agencies.\n\n\nGen3 is managed by the Center for Data Intensive Science (CDIS) at the University of Chicago.\n\n\nFor more information visit:  \nCDIS Guiding Principles\n\n\n Support \n\n\n\nOperation of the Gen3 commons is supported by generous grants from Amazon Web Services' \nGrants for Research Credit Program\n and Microsoft Azure's \nResearch Grant Program\n.\n\n\n\n\nThe Data Commons Architecture\n\uf0c1\n\n\n\n\nUser access to the Gen3 data commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.  \n\n\nOther secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.\n\n\n Diagram of the System Architecture \n\n\n\n\n\n\n\nContact Gen3\n\uf0c1\n\n\n\n\n\n\n\n\nLocations:\n\n\n\n\nShoreland\n:\n    5454 South Shore Drive\n    Suite 2B\n    Chicago, IL 60615\n\n\n\nUniversity of Chicago\n:\n    900 East 57th Street\n    10th Floor, Room 10148\n    Chicago, IL 60616\n\n\n\n\n\n\n\n\n\nEmail:\n\n\n\n\nGeneral Inquiries: \n\n\nTechnical Support:", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-gen3-documentation", 
            "text": "", 
            "title": "Welcome to Gen3 Documentation"
        }, 
        {
            "location": "/#the-data-commons-architecture", 
            "text": "User access to the Gen3 data commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.    Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.", 
            "title": "The Data Commons Architecture"
        }, 
        {
            "location": "/#contact-gen3", 
            "text": "Locations:   Shoreland :\n    5454 South Shore Drive\n    Suite 2B\n    Chicago, IL 60615  University of Chicago :\n    900 East 57th Street\n    10th Floor, Room 10148\n    Chicago, IL 60616     Email:   General Inquiries:   Technical Support:", 
            "title": "Contact Gen3"
        }, 
        {
            "location": "/user-guide/guide-overview/", 
            "text": "User Guide Overview\n\uf0c1\n\n\n\n\nThis documentation includes the following user guides for assisting contributors with submitting their data to the Gen3 data commons and accessing their data:\n\n\n\n\n\nHow to submit your data to the Gen3 data commons\n\n\nHow to query, access, and analyze data in the Gen3 commons\n\n\n\n\n\n\nAppendices\n\uf0c1\n\n\n\n\n\n\nWorking with the API\n\n\nWorking with the Proxy and Whitelist\n\n\nMinimum Technical Data Elements (MTDE)\n\n\nData Dictionary Viewer\n\n\nTemplate Metadata TSVs\n\n\nManaging Submission Timepoints", 
            "title": "Guide Overview"
        }, 
        {
            "location": "/user-guide/guide-overview/#user-guide-overview", 
            "text": "This documentation includes the following user guides for assisting contributors with submitting their data to the Gen3 data commons and accessing their data:   How to submit your data to the Gen3 data commons  How to query, access, and analyze data in the Gen3 commons", 
            "title": "User Guide Overview"
        }, 
        {
            "location": "/user-guide/guide-overview/#appendices", 
            "text": "Working with the API  Working with the Proxy and Whitelist  Minimum Technical Data Elements (MTDE)  Data Dictionary Viewer  Template Metadata TSVs  Managing Submission Timepoints", 
            "title": "Appendices"
        }, 
        {
            "location": "/user-guide/data-contribution/", 
            "text": "Project Submission Overview\n\uf0c1\n\n\n\n\n\n\nSign documents, fill out forms, get credentials\n\n\nPrepare and submit metadata to commons API\n\n\nPrepare and submit full dataset (w/o metadata tsvs) to object storage.\n\n\n\n\nSteps to Contribute a Project to the Gen3 Commons\n\n\n\n\n\nReview and sign legal\n\n\nComplete the data inventory form\n\n\nReceive project name and access credentials\n\n\nPrepare metadata that fits the data model\n\n\nAccess metadata submission portal\n\n\nSubmit and validate project metadata tsvs\n\n\nGet and configure s3 data storage credentials\n\n\nUpload \"raw\" data to object storage\n\n\n\n\n* NOTE:  Gen3 members are encouraged to submit multiple projects to the commons.   To do so, repeat steps 2-8 above.\n\n\nWhy Do Gen3 Commons Use a Data Model?\n\n\n\n\n\nHaving all participating members use the same data model:\n\n\nAllows for standardized metadata elements across a commons.\n\n\nPermits flexible and scaleable API generation based on data commons software that reads the data model schema.   \n\n\nLets users query the commons API so that an ecosystem of applications can be built.\n\n\nHelps automate the validation of submitted data.   \n\n\n\n\n\n\n\n\nHere is the most \ncurrent data model\n.\n\n\nHere is the most current \ngraph of the model\n.\n\n\nIf you have an submission element that you believe can't be described in the model, we'd be glad to work with you to find a home for the element or update the model.   \n\n\n Questions? \n\n\n\nContact:\n\n\n\n\n\n\nFor Legal:  gen3-legal@occ-data.org\n\n\n\n\n\n\nFor Submission/Access Questions:   gen3-support@datacommons.io\n\n\n\n\n\n\n\n\n1. Review and sign legal agreement\n\uf0c1\n\n\n\n\nTo use the Gen3 commons, please review and sign the following agreements and return them to info@gen3.org.   The most current versions (and past versions for transparency) of all of the below documents can be found at:\n\n\nhttps://www.gen3.org/data-governance/\n\n\n\n\nData Contributor Agreement (DCA)\n\n\nData Services/Use Agreement (DUA)\n\n\n\n\nIf you only wish to contribute data, you do not need to sign the DUA.\n\nThese documents may also reference the:\n\n\n\n\nPrivacy and Security Agreement\n\n\nIntellectual Property Rights (IPR) Policy\n\n\n\n\n\n\n2. Complete the data inventory form\n\uf0c1\n\n\n\n\nPrepare a pre-submission \ndata inventory form\n.\n\n\nHaving this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form.  \n\n\n\n\n3. Receive project name / API credentials\n\uf0c1\n\n\n\n\nOnce you have completed the \ndata inventory form\n, you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud headnode), and instructions to access the metadata submission portal and an object storage for your project.  \n\n\nThe project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you \nsubmit the metadata\n for your experiment nodes.\n\n\nProject name example\n\n\n\nmycompanyname\n_P0001_T1\n\nmycompanyname\n_P0002_T1\n\nmycompanyname\n_P0001_T2\n\n\n\n\nBreakdown:\n\n\n\n\n\n\"\nmycompanyname>\" identifies the submitter organization\n\n\n\"P000x\" identifies the submission number for said organization in said train\n\n\n\"Tx\" identifies the train number\n\n\n\n\nNOTE:  the Gen3 data submission calendar is broken up into different trains.\nNOTE2:   Your project folder will have a prefix appended to it to identify the commons.  eg - Gen3:\n\n\nGen3_\nmycompanyname\n_P0001_T1\n\n\n\n\n\n\n\n4. Prepare metadata that fits the data model\n\uf0c1\n\n\n\n\n Overview \n\nGen3 data contributors will need to prepare metadata for their submission in tab-separated value (tsv) files, login to a portal provided for submission, and upload and validate their metadata submission.   This is simultaneously the most challenging and crucial part of contributing data to a Gen3 commons.   This page details the preparation and ordering of the tsvs that will be submitted.   The next two pages cover submission virtual machine (VM) access and uploading/validating your metadata tsv submission.   \n\n\n Review and understand the data model\n\n\n\nNow that you have your project name, you can begin building out the rest of your metadata. Reference the most recent \ngraph model of your commons\n to help guide you on the necessary components in your submission. As you can see, from project you build up to experiment then to case and so on. For the properties that are allowed within this submission, please take some time to read through the \ndictionary schemas\n. Descriptions for each property as well as the valid submission values can be found in those schemas.\n\n\nOnce you have \naccess to submission portal\n, we recommend using the \nData Dictionary Viewer\n to review the schema and determine which properties best describe your submission.   This tool will help you understand the field types, requirements, and node dependencies for your submission.\n\n\nCreate your TSVs\n\n\n\nIt may be helpful to think of each TSV as a node in the graph of the data model.   Each node can have multiple components, for example a project could have multiple experiments in a node.  \n\n\nBlank TSV templates can be found \nhere\n.\n\n\n\n\nNote there are wiki pages associated with each potential tsv or node in the templates. They show example fields and information about data provenance.  \n\n\nField types and limitations can be gleaned from a careful read of the associated \nyaml files\n.\n\n\n\n\nDetermine Submission Order\n\n\n\nBefore we discuss the actual submission process, we must first mention that the files must be submitted in a specific order. Once again referring back to the \ngraph model\n, you cannot submit a node without submitting the nodes to which it points.\n\n\nIf you submit a file out of order, the validator will reject the submission on the basis that the dependency you point to is not present (e.g. the read_groups.submitter_id in assay_result.tsv will be pointing to a node that doesn\u2019t exist).  The \nData Dictionary viewer\n can help you determine these dependencies.\n\n\nSample Diagram of TSV Submission Order\n\n\n\nWhile this diagram represents an earlier version of the Gen3 data model, the required submission logic for current versions of the model will be very similar.\n\n\n\n\n\n\n5. Access metadata submission portal\n\uf0c1\n\n\n\n\nWhat is the metadata submission portal?\n\n\n\nThe metadata submission portal is secure environment for submitting the tsv metadata files associated with your project submission, querying the metadata associated with your project and others, and using the Data Dictionary Viewer to understand fields.   You will be able to login with your OAuth and you will have access update or edit the metadata associated with your submission by adding tsv files or \"nodes\" to the graph.   \n\n\nWhere is the metadata submission portal?\n\n\n\nLinks to Gen3 Commons Submission portals:\n\n\nBloodPAC\n\n\n\n\n6. Submit and validate project metadata tsvs\n\uf0c1\n\n\n\n\n Begin your submission\n\n\n\nFrom the submission portal select the project for which you wish to submit metadata.   Remembering the order in which you need to submit your tsvs (see \nDetermine Submission Order\n for details) begin your submission by uploading or copying in your first tsv (likely \"experiment.tsv\").   NOTE:   If you would prefer submitting nodes in json to tsv the submission portal also accepts json.  \n\n\n\n\nTo get you started, the first node - \"project\" has already been created for you.\n\n\n\n\nNow you should see a lot of details about the submission process. At the very bottom, you should be able to immediately get a grasp of how the submission went with the following fields:\n\n\n{'entity_error_count': 0,\n'message': 'Transaction successful.',\n'success': True,\n'transaction_id': 403,\n'transactional_error_count': 0,\n'transactional_errors': [],\n'updated_entity_count': 40}\n\n\n\n\nSpecifically, the message and success fields should provide you with whether your submission was valid and went through. If you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes:\n\n\n{'action': 'update',\n 'errors': [],\n 'id': 'asdf21as-2q4a-2563-213k-8dn4kg8dsb3j',\n 'related_cases': [],\n 'type': 'case',\n 'unique_keys': [{'project_id': 'bpa-MyGroup_P0001_T1'\n   'submitter_id': u'BPA_MG_P0001_EX1_C1'}],\n 'valid': True,\n 'warnings': []},\n\n\n\n\nOn a successful submission, the API will return something like above. The action can be used to identify if the node was created new or just updated. Other useful information includes the id for the node. This is the UUID for the submission and is unique for the node throughout the entirety of the Gen3 Commons. The other unique_key provided is the tuple project_id and submitter_id; another way of saying that the submitter_id combined with the project_id is a universal identifier for this node.\n\n\n Troubleshooting and finishing your submission\n\n\n\nIf, against all odds, your submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission.\n\n\nThe best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what tripped our validation check. The most common problems are simple issues such as spelling errors or mislabeled fields.\n\n\nOther errors include the one I mentioned earlier about submitting out of order and errors from not adhering to valid values as defined in our dictionary.\n\n\nIf you don't receive any output it usually means your TSV is invalid and must be edited.\n\n\n Submission Failure Example \n\n\n\n\n\n ALTERNATIVE in BETA:  Form submission \n\n\n\nUsing tsvs allows users to bulk submit their metadata.   This is ultimately a much faster process for users with larger datasets.   If you wish, you can also use form submission to insert properties into a node.   \n\n\nNOTE:   if you use the same submitter ID, and submit again to the same node, it will overwrite properties.   Be sure to change it should you choose to use form submission multiple times on a given node.   \n\n\nNOTE2:   it is not currently possible to remove values submitted in error using the form submission method.\n\n\n\n\n Query the API \n\n\n\nIf helpful you can query the API in the submission portal to confirm or correct your submission, to delete nodes, or to submit json queries to learn more about your submission or all the data in the commons.   To learn more visit the \nAPI section of the wiki\n.\n\n\nCheck the commons matrices\n\n\n\nRead \n\"What's an example of the API at work?\"\n to learn more about how the \ndata matrices\n work.   Suffice to say, you can check them hourly to see relevant metadata you submit appear!\n\n\nProvide feedback\n\n\n\nYou may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the Gen3 support team at gen3-support@occ-data.org. We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values.\n\n\nLet us know submission is complete\n\n\n\nPlease contact the Gen3 support team to let us know when your submission is complete.\n\n\nSubmission FAQ\n\n\n\n How do I know my submission was accepted? \n\n\n\nYou will see this within the json output:\n\n\n'message': 'Transaction successful.',\n'success': True,\n\n\n\n\nHow can I learn more about the elements of my existing submission?\n\n\n\nWhen you are viewing a project, there is a \"browse nodes\" feature.   From here you can download, view, or completely delete the tsvs associated with any project you have write access to.\n\n\nWhat happens if i need to resubmit a tsv that was already accepted?\n\n\n\nWhen you resubmit (aka submit to a node with the same submitter id), you will update the existing node.\n\n\nFor example, if you submit a sample with \nsample_type\n, \ncomposition\n, and \ntube_type\n, and you later realize that the tube type was wrong for that submission, if you were to resubmit a tsv that has just the \nsubmitter_id\n and \ntube_type\n it will overwrite ONLY the tube type. The \nsample_type\n and composition from the previous submission will still be in the database.  What this means is that if you had previously submitted something you DON'T want, like say you realize that you accidentally submitted the \ntube_type\n as \nmethod_of_sample_procurement\n, simply renaming the header in your TSV would not overwrite the existing data in the node.  You would need to submit null/empty values for \nmethod_of_sample_procurement\n to get rid of it.\n\n\n I was part of the very first submission group, in Nov/Dec 2016 when we added TSVs directly into an object store.   Where is the `project.submitter_id` field? \n\n\n\nprojects.submitter_id\n has become \nprojects.code\n now.   SO:\n\n\nEXAMPLE:\n\n \nprogram.name\n = 'bpa'\n\n \nproject.code\n ='MyOrg_P0001_T1'\n* \nproject_id\n = \nprogram.name\n + '-' + \nproject.code\n = 'bpa-MyOrg_P0001_T1'\n\n\n\n\n7. Get and configure s3 data storage credentials\n\uf0c1\n\n\n\n\nOverview\n\n\n\nNow that you've successfully submitted and validated your project metadata, it's time to upload your 'raw' data to the Gen3 Commons.   The Gen3 commons utilize \nobject storage\n. \n\n\n\nThis page details how users gain and manage the credentials to access a project folder.   The \nfollowing page\n will detail submitting data to the project folder.   \n\n\n\n\nWhy should we use command line to submit data instead of some kind of website?\n\n\n\n\nBecause for transfer of large files or many files, there can be time-out issues, encryption issues, or corruption issues.   Using the command line ensures secure and complete transfer.\n\n\n\n\nNOTE:  if you only have small files and are uncomfortable operating on the command line, you may want to try using a GUI tool like \nCyberduck\n to connect to S3 and manage your upload instead.  \n\n\n\n\n Obtain S3 credentials \n\n\n\n\n\nGo to https://bionimbus-pdc.opensciencedatacloud.org/storage/ Select and use the authentication method you gave in the \ndata inventory form\n\n\nDownload access keys by selecting the appropriate button next to the information for your username, e.g., under \"Available BPA Datasets\". The button is labeled \u201cGenerate S3 credential.\u201d See Figure 1.\n\n\nCredentials appear as comma-separated values including an access key and a secret key. The secret key should remain secret.  Do not share these keys.\n\n\n\n\n\n\nNOTE:   Do not share the s3 credentials you gain below with anyone.  \n\n\nIf you need to obtain new credentials, repeat steps (2.a) through (2.c). The button will now say \u201cRotate key\u201d button, which will deactivate previous credentials and provide new ones.\n\n\n\n\n\n\n\n\nKNOWN ISSUE: Safari will not provide S3 credentials on the first try. After generating keys once, press the \u201cRotate key\u201d button.\n\n\n\n\n Install AWS CLI \n\n\n\n Mac OSX or Linux/Unix\n\n\n\nOpen your \u201cTerminal\u201d application and type the following command:\n\n\nsudo apt-get install awscli\n\n\n\n\nor\n\n\nsudo easy_install awscli\n\n\n\n\nIf prompted, enter your device\u2019s password.\n\n\n Windows \n\n\n\nDownload and run one of the windows installers at: \nhttps://aws.amazon.com/cli/\n\n\n Configure AWS CLI with the downloaded project credentials \n\n\n\nIn your \u201cTerminal\u201d application, type the line below, replacing \u201cCREATE YOUR PROFILE NAME\u201d with a profile name you create.\n\n\naws configure --profile \nCREATE YOUR PROFILE NAME\n\n\n\n\n\n\n\nPro-tip:\n  Profile management will be very important as groups submit multiple projects.   Your downloaded s3 credentials only give you access to the project folder for which they are created.   If you are only submitting one project, you don't need profiles.   If you are submitting multiple, you will want to carefully pick project names for each project.   eg - P0001_T1 and P0002_T1.\n\n\n\n\nSome good sources on managing multiple profiles:\n\n\nOSX/Linux: \nhttp://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-multiple-profiles\n\nWindows powershell: \nhttp://docs.aws.amazon.com/powershell/latest/userguide/specifying-your-aws-credentials.html\n\n\n\n\n\n\nAfter pressing Enter/Return, you will be prompted for your Access Key ID and Secret Access Key. These are the S3 credentials you downloaded earlier. Enter keys as prompted, pressing Enter/Return after each step.\n\n\nAWS Access Key ID [****************]:\nAWS Secret Access Key [****************]:\n\n\n\n\nEnter a default region name, as prompted - us-east-1. Press Enter/Return.\n\n\nDefault region name [None]: us-east-1\n\n\n\n\nPress return at the final prompt.\n\n\nDefault output format [None]:\n\n\n\n\nYou should now be able to see the folder names in the bpa-data bucket.   \n\n\naws s3 ls s3://bpa-data/ --profile \nprofilename\n\n\n\n\n\n\n\n8. Upload \"raw\" data to object storage\n\uf0c1\n\n\n\n\n Preparing your data \n\n\n\nData files such as BAMs, FASTQs, or PDFs should be uploaded directly to the object store.  The metadata TSVs you prepared should not be submitted to the object store, as they have already been submitted via the API.\n\n\nPlease prepare a single folder with all of submission files in the same directory.   No sub directories.   \n\n\n Uploading your data \n\n\n\nYou can now upload all the files in the prepared folder on your local computer using the AWS CLI and the profiles you configured in \nstep 7\n. Below is the command to copy a folder filled with all your submission files on your local computer to your project folder in a commons. You need to change the name of /path/folder/ to the name of the path of the folder you want to upload.\n\n\naws s3 cp --sse AES256 [/path/folder/] s3://bpa-data/[foldername] --recursive --profile [profilename]\n\n\n\n\n\n\nEXTRA:  In an object store, a \"folder\" or \"dir\" can't exist with nothing in it.   Thus, if you were to 'ls' before moving any files into it, you wouldn't see the project folder you have access to.   In the example above you're essentially uploading all your data and \"creating a folder\" in a single step.   \n\n\n\n\nOther useful commands and AWS CLI documentation can be found at:\n \nhttps://www.opensciencedatacloud.org/support/pdc.html\n and\n \nhttps://aws.amazon.com/cli/\n\n\n\n\nAppendix: Data Dictionary Viewer\n\uf0c1\n\n\n\n\nThe \nData Dictionary Viewer\n is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field.   Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: \nhttps://data.Gen3.org/dd/\n\n\nThe Data Dictionary Viewer lets you browse and understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.   \n\n\nIn addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.  \n\n\n\n\n\n\nAppendix: Managing timepoints in a submission\n\uf0c1\n\n\n\n\n\nSome elements of submitted datasets could be related to each other in time.   To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates.   Every other date field is anchored by the \"index_date\" in the \"case\" node.\n\n\n\nIn this field you can have things like \"Study Enrollment\" or \"Diagnosis\".   Study the case node in the dictionary for more information on the index_date field:  \nhttps://data.Gen3.org/dd/case\n\n\n Examples of submissions using multiple date times\n\n\n\nPatient A enrolls in a study on July 1, and has a blood sample taken on July 10. For patient A you would report:\n\n case.index_date = \"Study Enrollment\"\n\n biospecimen.days_to_procurement = July 10 - July 1 = 9\n\n\nAlternatively if they were diagnosed 2 years before the study began and you wanted to use that as the index_date nothing is stopping you:\n\n case.index_date = \"Diagnosis\"\n\n biospecimen.days_to_procurment = July 10, 2017 - July 1, 2015 =  739\n\n\n Negative Dates \n\n\n\nDays to values can also be negative. If you have an index_date event that occurs after the event, you would present those days_to values as negative. If Patient A had a biospecimen taken when they were initially diagnosed:\n\n case.index_date = \"Study Enrollment\"\n\n biospecimen.days_to_procurement = July 10, 2015 - July 1, 2017 = -721\n\n\n No Time Series \n\n\n\nThe days_to_procurement and days_to_collection are required fields. If you do not have any data for these, we allow escape values of \"Unknown\" and \"Not Applicable\". Please use \"Unknown\" in the instances where you have established a time series but are unable to pin down the date of the event. Use \"Not Applicable\" if you do not have a time series at all.\n\n\nAppendix: Minimum Technical Data Elements\n\uf0c1\n\n\nTo facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the \n\u201cMinimum Technical Data Elements\u201d (MTDE)\n.   These pre-analytic fields were determined through an iterative series of conversations with data contributors and the \u201cData Experience\u201d Gen3 group.  \n\n\nFor example:\n\n\nBiospecimen Node\n\n\n* Blood Tube Type\n* Procurement Temperature\n* Shipment Temperature\n\n\n\nSample Node\n\n\n* Composition\n* Blood Fractionation Method\n* Hours to Fractionation Upper/Lower\n    * From blood draw to completion of fractionation\n\n\n\nAliquot Node\n\n\n* Clinical or Contrived\n* Hours to Freezer Upper/Lower\n    * From completion of fractionation to freezing\n* Storage Temperature\n\n\n\nAnalyte Node\n\n\n* Analyte Isolation Method\n\n\n\nQuantification Assay Node\n\n\n* Assay Method\n* DNA quantification method", 
            "title": "Data Contribution"
        }, 
        {
            "location": "/user-guide/data-contribution/#project-submission-overview", 
            "text": "Sign documents, fill out forms, get credentials  Prepare and submit metadata to commons API  Prepare and submit full dataset (w/o metadata tsvs) to object storage.", 
            "title": "Project Submission Overview"
        }, 
        {
            "location": "/user-guide/data-contribution/#1-review-and-sign-legal-agreement", 
            "text": "To use the Gen3 commons, please review and sign the following agreements and return them to info@gen3.org.   The most current versions (and past versions for transparency) of all of the below documents can be found at:  https://www.gen3.org/data-governance/   Data Contributor Agreement (DCA)  Data Services/Use Agreement (DUA)   If you only wish to contribute data, you do not need to sign the DUA. \nThese documents may also reference the:   Privacy and Security Agreement  Intellectual Property Rights (IPR) Policy", 
            "title": "1. Review and sign legal agreement"
        }, 
        {
            "location": "/user-guide/data-contribution/#2-complete-the-data-inventory-form", 
            "text": "Prepare a pre-submission  data inventory form .  Having this information helps the Gen3 submission team prepare for your project and setup your storage access credentials using the authentication method you provide in the form.", 
            "title": "2. Complete the data inventory form"
        }, 
        {
            "location": "/user-guide/data-contribution/#3-receive-project-name-api-credentials", 
            "text": "Once you have completed the  data inventory form , you will receive an email with your project name (associated with project data), username (used to login to Virtual Private Cloud headnode), and instructions to access the metadata submission portal and an object storage for your project.    The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you  submit the metadata  for your experiment nodes.", 
            "title": "3. Receive project name / API credentials"
        }, 
        {
            "location": "/user-guide/data-contribution/#4-prepare-metadata-that-fits-the-data-model", 
            "text": "", 
            "title": "4. Prepare metadata that fits the data model"
        }, 
        {
            "location": "/user-guide/data-contribution/#5-access-metadata-submission-portal", 
            "text": "", 
            "title": "5. Access metadata submission portal"
        }, 
        {
            "location": "/user-guide/data-contribution/#6-submit-and-validate-project-metadata-tsvs", 
            "text": "", 
            "title": "6. Submit and validate project metadata tsvs"
        }, 
        {
            "location": "/user-guide/data-contribution/#7-get-and-configure-s3-data-storage-credentials", 
            "text": "", 
            "title": "7. Get and configure s3 data storage credentials"
        }, 
        {
            "location": "/user-guide/data-contribution/#8-upload-raw-data-to-object-storage", 
            "text": "", 
            "title": "8. Upload \"raw\" data to object storage"
        }, 
        {
            "location": "/user-guide/data-contribution/#appendix-data-dictionary-viewer", 
            "text": "The  Data Dictionary Viewer  is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field.   Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at:  https://data.Gen3.org/dd/  The Data Dictionary Viewer lets you browse and understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.     In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.", 
            "title": "Appendix: Data Dictionary Viewer"
        }, 
        {
            "location": "/user-guide/data-contribution/#appendix-managing-timepoints-in-a-submission", 
            "text": "Some elements of submitted datasets could be related to each other in time.   To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates.   Every other date field is anchored by the \"index_date\" in the \"case\" node.  \nIn this field you can have things like \"Study Enrollment\" or \"Diagnosis\".   Study the case node in the dictionary for more information on the index_date field:   https://data.Gen3.org/dd/case", 
            "title": "Appendix: Managing timepoints in a submission"
        }, 
        {
            "location": "/user-guide/data-contribution/#appendix-minimum-technical-data-elements", 
            "text": "To facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the  \u201cMinimum Technical Data Elements\u201d (MTDE) .   These pre-analytic fields were determined through an iterative series of conversations with data contributors and the \u201cData Experience\u201d Gen3 group.    For example:  Biospecimen Node  * Blood Tube Type\n* Procurement Temperature\n* Shipment Temperature  Sample Node  * Composition\n* Blood Fractionation Method\n* Hours to Fractionation Upper/Lower\n    * From blood draw to completion of fractionation  Aliquot Node  * Clinical or Contrived\n* Hours to Freezer Upper/Lower\n    * From completion of fractionation to freezing\n* Storage Temperature  Analyte Node  * Analyte Isolation Method  Quantification Assay Node  * Assay Method\n* DNA quantification method", 
            "title": "Appendix: Minimum Technical Data Elements"
        }, 
        {
            "location": "/user-guide/data-access/", 
            "text": "Data Access Overview\n\uf0c1\n\n\n\n\n1. Send credentials and get welcome email\n\uf0c1\n\n\n\n\nSend SSH Key and Oauth to Gen3 commons team.\n\n\nTo access the VPC, users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to \n.   \n\n\n\n\nNOTE:   Do not send your private ssh key.   This is confidential and should never be shared with anyone.  \n\n\n\n\nThe pubkey will be used to access the login node and any VMs setup for the user.    The email will be used to permit access to:\n\n\n\n\nBionimbus\n to receive s3 data storage credentials\n\n\ndata.Gen3.org\n for browser based querying of metadata.\n\n\n\n\n\n\nNOTE:  for Gen3 members that were also their organization's contact for data submission, you already have access to your s3 credentials and the commons metadata API \n.   Just send your pubkey.  \n\n\n\n\n I'm not familiar with SSH - how do I generate a keypair? \n\n\n\nGithub has a very nice \nssh tutorial\n with step-by-step instructions for Mac, Windows (users with gitbash installed), and Linux users.   If you're new to using SSH we'd recommend reviewing the links:\n\n\n\n\nAbout SSH\n\n\nChecking for Existing SSH Keys\n\n\nGenerating a new SSH key and adding it to the SSH Agent\n\n\n\n\n\n\nNOTE:  For Windows users, we recommend installing \nGit for Windows\n and using the Git Bash feature to interact on the command line, manage ssh keys, and s3 credentials.   \n\n\n\n\nReceive a welcome email\n\n\n\nGen3 members with the appropriate signed legal documents will be sent an email that gives the following unique information:\n\n\n\n\nusername (this is used to ssh to the VPC login node) - eg:  \nssh -A \nusername\n@34.197.164.18\n\n\nan IP associated with your new VM\n\n\n\n\n\n\n2. SSH to Virtual Machine: config\n\uf0c1\n\n\n\n\nHow will I access the Login Node and my Virtual Machine (VM)?\n\n\n\nGen3 Commons users will login to the Virtual Private Cloud (VPC) headnode, then hop over to their analysis VM.   For more information on the \nVPC architecture\n.\n\n\nIn your \nwelcome email\n you received your username and your vm.   In order to access your VM, you first must access the VPC login node.   This configuration helps ensure the security of the BloodPAC commons by having your VM in a private subnet.   Using the credentials from your welcome email this can be done in the following order:\n\n\n\n\nSSH to login node:   \nssh -A \nusername\n@34.197.164.18\n\n\nSSH from login node to your VM:  \nssh -A ubuntu@\nmy_VM_ip\n\n\n\n\n\n\nNOTE 1:   \n34.197.164.18\n is the IP for the login node.   This is unlikely to change.   \n\n\nNOTE 2:  the \n-A\n flag forwards all the keys in your key chain.   For more details on managing SSH keys, check the guides linked in the \nprevious step\n.\n\n\nNOTE 3:   You can't login to your analysis VM (in the private subnet) without first logging in to the login node (in the public subnet).  \n\n\n\n\nAdvanced users can manage these connections however they see fit.   For other users, we recommend updating your SSH config file so you can setup a 'multihop' ssh tunnel.  To 'multihop' in this context is to use a single command to get to your VM.    What follows are instructions for updating your \n.ssh/config\n file to get to your VM in a single command.\n\n\n\n\n3. Setting up an ssh config for 'multihop'\n\uf0c1\n\n\n\n\nTo start, go to your .ssh directory in your laptop home directory.\n\n\ncd ~/.ssh\n\n\n\n\nIf this directory does not exist, make one.\n\n\nmkdir -p ~/.ssh\ncd ~/.ssh\n\n\n\n\nWithin this directory create a file named \"config\" [Note: I use vim here but any text editor will do]:\n\n\nvim config\n\n\n\n\nIn this file you can specify various hosts for access via ssh. Your host for the BPA head node should look something like this:\n\n\nHost BPA\n    Hostname 34.197.164.18\n    User YOUR_USERNAME\n    IdentityFile /path/to/YOUR_CREDFILE\n    ForwardAgent yes\n\n\n\n\nWhere /path/to/YOUR_CREDFILE might be, e.g., \n~/.ssh/id_rsa\n\n\nThe username will be provided to you by the BloodPAC support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the BloodPAC head-node using this host:\n\n\nssh BPA\n\n\n\n\nExit the BPA head node and return to your local machine. Back in your config file, add a new host for the BPA submission VM:\n\n\nHost analysis\n    Hostname YOUR_VM_IP\n    User ubuntu\n    ProxyCommand ssh -q -AXY BPA -W %h:%p\n\n\n\n\nOnce again you will receive the Hostname IP from the BloodPAC support team in step 4.  This host will route you through the BPA head node and take you directly to your personal BloodPAC analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM:\n\n\nssh analysis\n\n\n\n\nIf you've done everything correctly, you should now be in the analysis VM.  \n\n\n\n\n4. Access \"raw\" data storage from Virtual Machine\n\uf0c1\n\n\n\n\n Add s3 'raw' data storage credentials to your VM \n\n\n\nNow you'll need to add your storage credentials to your analysis VM.   Details on getting your credentials from the \nBionimbus storage portal\n are outlined in the \ndata contribution section\n of this documentation.   If you are only accessing data and did not contribute, please follow those directions to acquire your keys using the Oauth you provided in \nStep 1\n of the Data Access section.   \n\n\nIf you did contribute data you should use an existing key.   They will still have \"read/write\" permission to your project folder, but will also have permission to \"read\" other data in the BloodPAC commons.   If you submitted multiple projects and have multiple keys, all will have the same \"read\" permissions for data - it only matters which one you pick if you still intend to write data to your project folder from your VM.  \n\n\nAs a reminder, the command to setup a profile is:\n\naws configure --profile \nCREATE YOUR PROFILE NAME\n\n\n Example:  Configure an s3 profile in your VM \n\n\n\n\n\nReview contents of the Gen3 Commons\n\n\n\nYou can now review the 'raw' data folders in the Gen3 object storage.   \n\n\naws s3 ls s3://commons-data/ --profile \nprofilename\n\n\nTo peek inside a folder:\n\n\naws s3 ls s3://commons-data/\nfoldername\n/ --profile \nprofilename\n\n\nYou can use other commands to pull files from the s3 object storage into your VM. If you're not familiar with AWScli commands, we recommend reviewing the \nhigh-level docs\n or the \ncomplete documentation\n.\n\n\n\n\nNOTE:   Remember, that since your access is read only (except any projects you've submitted associated with your keys), you will only be able to read, not write to the project folders in the commons. \n\nNOTE2:   If you are using keys with write access to your project folders in the commons, be VERY CAREFUL.   Don't delete any data you'll have to resubmit later.      \n\n\n\n\n Ready to work! \n\n\n\nYou're ready to use whatever tools you wish to analyze data in the commons within your VM.   For requests for alternative configurations, analysis storage, or other needs please contact \n.\n\n\nFor an example of how you could use a Jupyter Notebook to run analysis in the browser on your local computer, please continue on to the \nnext section\n.    There are lots of good examples that may be useful to you.", 
            "title": "Data Access"
        }, 
        {
            "location": "/user-guide/data-access/#data-access-overview", 
            "text": "", 
            "title": "Data Access Overview"
        }, 
        {
            "location": "/user-guide/data-access/#1-send-credentials-and-get-welcome-email", 
            "text": "", 
            "title": "1. Send credentials and get welcome email"
        }, 
        {
            "location": "/user-guide/data-access/#2-ssh-to-virtual-machine-config", 
            "text": "", 
            "title": "2. SSH to Virtual Machine: config"
        }, 
        {
            "location": "/user-guide/data-access/#3-setting-up-an-ssh-config-for-multihop", 
            "text": "To start, go to your .ssh directory in your laptop home directory.  cd ~/.ssh  If this directory does not exist, make one.  mkdir -p ~/.ssh\ncd ~/.ssh  Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]:  vim config  In this file you can specify various hosts for access via ssh. Your host for the BPA head node should look something like this:  Host BPA\n    Hostname 34.197.164.18\n    User YOUR_USERNAME\n    IdentityFile /path/to/YOUR_CREDFILE\n    ForwardAgent yes  Where /path/to/YOUR_CREDFILE might be, e.g.,  ~/.ssh/id_rsa  The username will be provided to you by the BloodPAC support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the BloodPAC head-node using this host:  ssh BPA  Exit the BPA head node and return to your local machine. Back in your config file, add a new host for the BPA submission VM:  Host analysis\n    Hostname YOUR_VM_IP\n    User ubuntu\n    ProxyCommand ssh -q -AXY BPA -W %h:%p  Once again you will receive the Hostname IP from the BloodPAC support team in step 4.  This host will route you through the BPA head node and take you directly to your personal BloodPAC analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM:  ssh analysis  If you've done everything correctly, you should now be in the analysis VM.", 
            "title": "3. Setting up an ssh config for 'multihop'"
        }, 
        {
            "location": "/user-guide/data-access/#4-access-raw-data-storage-from-virtual-machine", 
            "text": "", 
            "title": "4. Access \"raw\" data storage from Virtual Machine"
        }, 
        {
            "location": "/appendices/data-dictionary/", 
            "text": "Data Dictionary Viewer\n\uf0c1\n\n\n\n\n\n\n\n\nThe Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.   \n\n\n\n\n\n\nIn addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.  \n\n\n\n\n\n\nGen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: \nhttps://data.Gen3.org/dd/\n\n\n\n\n\n\n\n\n\n\nViewing data dictionary as a graph:\n\uf0c1\n\n\n\n\nToggling the graph view:\n\uf0c1\n\n\n\n\nViewing data dictionary as tables:\n\uf0c1\n\n\n\n\nToggling between different views\n\uf0c1", 
            "title": "Data Dictionary"
        }, 
        {
            "location": "/appendices/data-dictionary/#data-dictionary-viewer", 
            "text": "The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.       In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.      Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at:  https://data.Gen3.org/dd/", 
            "title": "Data Dictionary Viewer"
        }, 
        {
            "location": "/appendices/data-dictionary/#viewing-data-dictionary-as-a-graph", 
            "text": "", 
            "title": "Viewing data dictionary as a graph:"
        }, 
        {
            "location": "/appendices/data-dictionary/#toggling-the-graph-view", 
            "text": "", 
            "title": "Toggling the graph view:"
        }, 
        {
            "location": "/appendices/data-dictionary/#viewing-data-dictionary-as-tables", 
            "text": "", 
            "title": "Viewing data dictionary as tables:"
        }, 
        {
            "location": "/appendices/data-dictionary/#toggling-between-different-views", 
            "text": "", 
            "title": "Toggling between different views"
        }, 
        {
            "location": "/appendices/mtde/", 
            "text": "Minimum Technical Data Elements (MTDE)\n\uf0c1\n\n\n\n\nTo facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the \u201cMinimum Technical Data Elements\u201d.   These pre-analytic fields were determined through an iterative series of conversations with data contributors and the Gen3 \u201cData Experience\u201d group.  \n\n\nFor example, if a data commons was designed to accept gene/genome sequences, in order to facilitate analysis of genetic data across studies submitted by different groups, a node called \"submitted aligned reads\" may require the following metadata properties (MTDE):  \n\n\nDownload template: \nJSON\n | \nTSV", 
            "title": "Minimal Technical Data Elements"
        }, 
        {
            "location": "/appendices/mtde/#minimum-technical-data-elements-mtde", 
            "text": "To facilitate cross analysis and improve the usability of the Gen3 commons, all data contributors should submit what are considered the \u201cMinimum Technical Data Elements\u201d.   These pre-analytic fields were determined through an iterative series of conversations with data contributors and the Gen3 \u201cData Experience\u201d group.    For example, if a data commons was designed to accept gene/genome sequences, in order to facilitate analysis of genetic data across studies submitted by different groups, a node called \"submitted aligned reads\" may require the following metadata properties (MTDE):    Download template:  JSON  |  TSV", 
            "title": "Minimum Technical Data Elements (MTDE)"
        }, 
        {
            "location": "/appendices/proxy-whitelist/", 
            "text": "Working with the proxy and whitelists\n\uf0c1\n\n\n\n\n Working with the Proxy \n\n\n\nTo prevent unauthorized traffic, the Gen3 VPC utilizes a proxy.   If you are using one of the custom VMs setup, there is already a line in your .bashrc file to handle traffic requests.   \n\n\nexport http_proxy=http://cloud-proxy.internal.io:3128\nexport https_proxy=$http_proxy\n\n\n\n\nAlternatively, if you have a different service or a tool that needs to call out, you can set the proxy with each command.  \n\n\nhttps_proxy=https://cloud-proxy.internal.io:3128 aws s3 ls s3://bpa-data/ --profile \nprofilename\n\n\n\n\n\n Whitelists \n\n\n\nAdditionally, to aid Gen3 Commons security, tool installation from outside sources is managed through a whitelist.   If you have problems installing a tool you need for your work, contact \n and with a list of any sites you might wish to install tools from.    After passing a security review,  these can be added to the whitelist to facilitate access.", 
            "title": "Working with the Proxy and Whitelist"
        }, 
        {
            "location": "/appendices/proxy-whitelist/#working-with-the-proxy-and-whitelists", 
            "text": "", 
            "title": "Working with the proxy and whitelists"
        }, 
        {
            "location": "/appendices/api/", 
            "text": "Working with the API\n\uf0c1\n\n\n\n\nWhat does the API do?\n\uf0c1\n\n\nThe API is created programmatically based on the \nGen3 commons data model\n.   All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps \n4-6 in the Data Contribution section\n).   \n\n\nWith the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons.   The API can be queried programmatically or through provided tools like the submission portal.  \n\n\nWe use \nGraphQL\n to manage the metadata in the Gen3 commons.  To learn the basics of writing queries in Graph QL, we recommend \nthis introduction\n.\n\n\nWhat's an example of the API at work?\n\uf0c1\n\n\nThe Gen3 commons team has created a few matrices to help describe submitted data.   These are linked to from: \nhttps://www.gen3.org/data-group/\n\n\nThese query the API for the desired metadata and return the matrices.   They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear.   If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.   \n\n\nSECRETS!   Credentials to query\n\uf0c1\n\n\nLike your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you query the API.    For users granted data access, these are in a .secrets file in the home directory of your VM.   These are used for every API call.   \n\n\nFor example:   If you're doing the \nJupyter notebook demo\n from your analysis VM, your secrets file is loaded early in the demo so you can query.   If you're using the submission portal to query, your secrets file is setup in a DB associated with your login credentials.  \n\n\nIf you receive an error like \"You don't have access to this data\", then you will most likely need to update your API keys and enter them into your VM's .secrets file.\n\n\n1) Click \"Create access key\" button:\n\n\n\n2) Copy the displayed keys:\n\n\n\n3) In your VM, open your .secrets file with a text editor:\n\n\n\n4) Paste your new keys into the .secrets file and save it:\n\n\n\nQueries in the submission portal\n\uf0c1\n\n\nYou can run queries directly in the submission portal by clicking the magnifying glass or directly at: \nhttps://data.gen3.org/graphql\n.    Queries are essential as you begin analysis.   The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need.\n\n\nExample templates have been setup \nhere\n.\n\n\n\n\nExtra:   Default = first 10 entries\n\uf0c1\n\n\nQueries by defult return the first 10 entries.   If you want more than that, you can specify it in the query call: \n(first:1000)\n\n\nIn the case that too many results are returned, you may receive a timeout error. In that case, you may want to use \npagination\n. For example, if there are 2,550 records returned, and your graphiQL query is timing out with \n(first:3000)\n, then break your query into multiple queries with offsets:\n\n\n(first:1000, offset:0)      # this will return records 0-1000\n(first:1000, offset:1000)   # this will return records 1000-2000\n(first:1000, offset:2000)   # this will return records 2000-2,550\n\n\n\n\nUpdating the example template \ndetails from experiment\n sample query to call the first 1000, the call becomes:  \n\n\n{\n    \nquery\n:\n query Test {\n        experiment (first:1000, submitter_id: \nINSERT submitter_id\n) {  \n            experimental_intent\n            experimental_description\n            number_samples_per_experimental_group\n            type_of_sample\n            type_of_specimen\n        }\n    } \n\n}\n\n\n\n\nBrowsing by project node\n\uf0c1\n\n\nThe metadata submission portal \nhttps://data.gen3.org/\n can be used to browse an individual submission by node.   Just select a project and then click the \"browse nodes\" button to the right.    From there, you'll get a screen like below where you can query by node in the dropdown at the left.\n\n\n Example:  Browse by node \n\n\n\n\n\nYou can also use this feature to download the tsv associated with the node, or if you have \"write\" access to the this project, delete existing nodes.   \n\n\nGraphing a project\n\uf0c1\n\n\nYou can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.  \n\n\n Example:  Graphing a project", 
            "title": "Working with the API"
        }, 
        {
            "location": "/appendices/api/#working-with-the-api", 
            "text": "", 
            "title": "Working with the API"
        }, 
        {
            "location": "/appendices/api/#what-does-the-api-do", 
            "text": "The API is created programmatically based on the  Gen3 commons data model .   All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps  4-6 in the Data Contribution section ).     With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons.   The API can be queried programmatically or through provided tools like the submission portal.    We use  GraphQL  to manage the metadata in the Gen3 commons.  To learn the basics of writing queries in Graph QL, we recommend  this introduction .", 
            "title": "What does the API do?"
        }, 
        {
            "location": "/appendices/api/#whats-an-example-of-the-api-at-work", 
            "text": "The Gen3 commons team has created a few matrices to help describe submitted data.   These are linked to from:  https://www.gen3.org/data-group/  These query the API for the desired metadata and return the matrices.   They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear.   If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.", 
            "title": "What's an example of the API at work?"
        }, 
        {
            "location": "/appendices/api/#secrets-credentials-to-query", 
            "text": "Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you query the API.    For users granted data access, these are in a .secrets file in the home directory of your VM.   These are used for every API call.     For example:   If you're doing the  Jupyter notebook demo  from your analysis VM, your secrets file is loaded early in the demo so you can query.   If you're using the submission portal to query, your secrets file is setup in a DB associated with your login credentials.    If you receive an error like \"You don't have access to this data\", then you will most likely need to update your API keys and enter them into your VM's .secrets file.  1) Click \"Create access key\" button:  2) Copy the displayed keys:  3) In your VM, open your .secrets file with a text editor:  4) Paste your new keys into the .secrets file and save it:", 
            "title": "SECRETS!   Credentials to query"
        }, 
        {
            "location": "/appendices/api/#queries-in-the-submission-portal", 
            "text": "You can run queries directly in the submission portal by clicking the magnifying glass or directly at:  https://data.gen3.org/graphql .    Queries are essential as you begin analysis.   The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need.  Example templates have been setup  here .", 
            "title": "Queries in the submission portal"
        }, 
        {
            "location": "/appendices/api/#extra-default-first-10-entries", 
            "text": "Queries by defult return the first 10 entries.   If you want more than that, you can specify it in the query call:  (first:1000)  In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use  pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with  (first:3000) , then break your query into multiple queries with offsets:  (first:1000, offset:0)      # this will return records 0-1000\n(first:1000, offset:1000)   # this will return records 1000-2000\n(first:1000, offset:2000)   # this will return records 2000-2,550  Updating the example template  details from experiment  sample query to call the first 1000, the call becomes:    {\n     query :  query Test {\n        experiment (first:1000, submitter_id:  INSERT submitter_id ) {  \n            experimental_intent\n            experimental_description\n            number_samples_per_experimental_group\n            type_of_sample\n            type_of_specimen\n        }\n    }  \n}", 
            "title": "Extra:   Default = first 10 entries"
        }, 
        {
            "location": "/appendices/api/#browsing-by-project-node", 
            "text": "The metadata submission portal  https://data.gen3.org/  can be used to browse an individual submission by node.   Just select a project and then click the \"browse nodes\" button to the right.    From there, you'll get a screen like below where you can query by node in the dropdown at the left.", 
            "title": "Browsing by project node"
        }, 
        {
            "location": "/appendices/api/#graphing-a-project", 
            "text": "You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.", 
            "title": "Graphing a project"
        }, 
        {
            "location": "/appendices/template-tsvs/", 
            "text": "Template TSVs for metadata submission\n\uf0c1\n\n\n\n\nThis folder contains all potential entities that could be submitted to the Gen3 data model. Some of the entities are optional.\n\n\nYou can find a complete and detailed description of the new data dictionary \nhere\n.\n\n\nA project will need to be created and you need to be \ngranted submitter access\n before you submit everything under a project.  You will start your submission from \nstudy.tsv\n.\n\n\nstudy.tsv\n\n\ncase.tsv\n\n\nbiospecimen.tsv\n\n\nsample.tsv\n\n\naliquot.tsv\n\n\nanalyte.tsv", 
            "title": "Template TSVs"
        }, 
        {
            "location": "/appendices/template-tsvs/#template-tsvs-for-metadata-submission", 
            "text": "This folder contains all potential entities that could be submitted to the Gen3 data model. Some of the entities are optional.  You can find a complete and detailed description of the new data dictionary  here .  A project will need to be created and you need to be  granted submitter access  before you submit everything under a project.  You will start your submission from  study.tsv .  study.tsv  case.tsv  biospecimen.tsv  sample.tsv  aliquot.tsv  analyte.tsv", 
            "title": "Template TSVs for metadata submission"
        }, 
        {
            "location": "/appendices/architecture/", 
            "text": "Gen3 Data Commons Architecture\n\uf0c1\n\n\n\n\nUser access to the Gen3 Commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.  \n\n\nOther secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.\n\n\nDiagram of the Gen3 System Architecture", 
            "title": "Data Commons Architecture"
        }, 
        {
            "location": "/appendices/architecture/#gen3-data-commons-architecture", 
            "text": "User access to the Gen3 Commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.    Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.", 
            "title": "Gen3 Data Commons Architecture"
        }, 
        {
            "location": "/user-guide/demo/", 
            "text": "DEMO: Using a Jupyter notebook for analysis\n\uf0c1\n\n\n\n\nThe bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses.    Both can be found in your VM in the analysis folder.    They can also be found at: \nhttps://github.com/occ-data/gen3-functions\n.    The Gen3 community is encouraged to add to the functions library or improve the notebook.  \n\n\n\n\nNOTE:   As the Gen3 community updates repositories, you can keep them up to date using \ngit pull origin master\n in the \nfunctions\n folder.   It has already been initialized to sync with this repository.\n\n\nNOTE2: If you receive an error when trying to do \ngit pull\n, you may need to set proxies and/or either save or drop any changes you've made:\n\n\n\n\n# set proxies:\nexport http_proxy=\nhttp://cloud-proxy.internal.io:3128\n\nexport https_proxy=\nhttp://cloud-proxy.internal.io:3128\n\n\n# to drop changes:\ngit stash save --keep-index\ngit stash drop\n\n# or save changes\ngit commit .\n\n# Update CDIS utils python libraries:\ngit clone https://github.com/uc-cdis/cdis-python-utils.git\ncd cdis-python-utils\nsudo -E python setup.py install\n\n# unset proxies to get juypter notebook to work again\nunset http_proxy;\nunset https_proxy;\n\n\n\n\nWhat follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser.   In the notebook, you'll learn about basic Gen3 commons operations like:  \n\n\n\n\nQuerying the API for metadata associated with submissions\n\n\nPulling data into your VM for analysis\n\n\nRunning a simple analysis over a file and collection of files\n\n\nPlotting the results\n\n\n\n\nIn the Jupyter notebook, many of the calls rely on more complex functions described in the \nanalysis_functions file\n.   It is worth taking the time to understand how this file works so you can use and customize it or build your own tools.    \n\n\nWe would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile.   Just contact \n for more information.\n\n\nRunning the notebook in your VM\n\uf0c1\n\n\nAfter we're logged in to our analysis VM and in the functions directory (from home: \ncd functions\n), run the jupyter notebook server.  \n\n\nRun the notebook server:\n\n\njupyter notebook --no-browser --port=8889\n\n\n\n\n\n\nNOTE:   You can stop a Juptyer server at anytime via \nctrl + c\n\n\n\n\nPort forwarding to your VM\n\uf0c1\n\n\nNext you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine.   \n\n\nOn a terminal session from your local machine (not in the VM) setup the connection:\n\n\nssh -N -L localhost:8888:localhost:8889 analysis\n\n\n\n\n\n\nNOTE:   In the example above \"analysis\" is the name of the ssh shortcut we \nsetup back in step 2\n.\n\n\n\n\nAccess the notebook in via browser\n\uf0c1\n\n\nIn your preferred browser and enter http://localhost:8888/;   Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.\n\n\n Example:   Run Server, port forward, access notebook in browser\n\n\n\n\n\nReview and follow the notebook\n\uf0c1\n\n\n Credentials \n\n\n\nThe notebook makes use of both a \n.secrets file\n that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the \ns3 profile you created\n to pull 'raw' data into your VM for analysis.\n\n\nThe first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is.       \n\n\n\n\nNOTE:  If you have forgotten what you called your profile, you can always take a look at the credential file to review.  From the VM run:  \nvi ~/.aws/credentials\n.  \n\n\n\n\n Jupyter Basics \n\n\n\nIf you're not familiar with Jupyter notebooks, you have a few options to run the commands inside.   You can review line by line (select cell - \nShift+Enter\n) or you can run all from the \"Kernel\" menu at the top of the browser.   \n\n\n Shutting Down your Server\n\n\n\nWhen you're done working, we encourage you to shut down your Jupyter server via \nctrl + c\n in the VM that's running it.  You don't have to do this every time, but you should do it when you don't need it any more.   \n\n\n VM Termination \n\n\n\nAt this point in the Gen3 commons development, you should contact \n when you no longer need your VM active.   Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.", 
            "title": "DEMO"
        }, 
        {
            "location": "/user-guide/demo/#demo-using-a-jupyter-notebook-for-analysis", 
            "text": "The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a starting python library and a sample analysis notebook to help jumpstart commons analyses.    Both can be found in your VM in the analysis folder.    They can also be found at:  https://github.com/occ-data/gen3-functions .    The Gen3 community is encouraged to add to the functions library or improve the notebook.     NOTE:   As the Gen3 community updates repositories, you can keep them up to date using  git pull origin master  in the  functions  folder.   It has already been initialized to sync with this repository.  NOTE2: If you receive an error when trying to do  git pull , you may need to set proxies and/or either save or drop any changes you've made:   # set proxies:\nexport http_proxy= http://cloud-proxy.internal.io:3128 \nexport https_proxy= http://cloud-proxy.internal.io:3128 \n\n# to drop changes:\ngit stash save --keep-index\ngit stash drop\n\n# or save changes\ngit commit .\n\n# Update CDIS utils python libraries:\ngit clone https://github.com/uc-cdis/cdis-python-utils.git\ncd cdis-python-utils\nsudo -E python setup.py install\n\n# unset proxies to get juypter notebook to work again\nunset http_proxy;\nunset https_proxy;  What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser.   In the notebook, you'll learn about basic Gen3 commons operations like:     Querying the API for metadata associated with submissions  Pulling data into your VM for analysis  Running a simple analysis over a file and collection of files  Plotting the results   In the Jupyter notebook, many of the calls rely on more complex functions described in the  analysis_functions file .   It is worth taking the time to understand how this file works so you can use and customize it or build your own tools.      We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile.   Just contact   for more information.", 
            "title": "DEMO: Using a Jupyter notebook for analysis"
        }, 
        {
            "location": "/user-guide/demo/#running-the-notebook-in-your-vm", 
            "text": "After we're logged in to our analysis VM and in the functions directory (from home:  cd functions ), run the jupyter notebook server.    Run the notebook server:  jupyter notebook --no-browser --port=8889   NOTE:   You can stop a Juptyer server at anytime via  ctrl + c", 
            "title": "Running the notebook in your VM"
        }, 
        {
            "location": "/user-guide/demo/#port-forwarding-to-your-vm", 
            "text": "Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine.     On a terminal session from your local machine (not in the VM) setup the connection:  ssh -N -L localhost:8888:localhost:8889 analysis   NOTE:   In the example above \"analysis\" is the name of the ssh shortcut we  setup back in step 2 .", 
            "title": "Port forwarding to your VM"
        }, 
        {
            "location": "/user-guide/demo/#access-the-notebook-in-via-browser", 
            "text": "In your preferred browser and enter http://localhost:8888/;   Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.", 
            "title": "Access the notebook in via browser"
        }, 
        {
            "location": "/user-guide/demo/#review-and-follow-the-notebook", 
            "text": "", 
            "title": "Review and follow the notebook"
        }
    ]
}