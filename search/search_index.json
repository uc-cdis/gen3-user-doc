{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Gen3 Documentation\n\uf0c1\n\n\n\n\n Overview \n\n\n\nThe Gen3 data commons allows approved researchers access to raw unprocessed datasets in a scalable, reproducible, privacy and security protected manner. This documentation describes the commons and provides a \nuser guide\n to assist contributors.\n\n\n Mission \n\n\n\nThe Gen3 commons was developed to accelerate scientific discovery through creation of a collaborative infrastructure that enables sharing of information between stakeholders in industry, academia, and regulatory agencies.\n\n\nGen3 is managed by the Center for Data Intensive Science (CDIS) at the University of Chicago.\n\n\nFor more information visit:  \nCDIS Guiding Principles\n\n\n Support \n\n\n\nOperation of the Gen3 commons is supported by generous grants from Amazon Web Services' \nGrants for Research Credit Program\n and Microsoft Azure's \nResearch Grant Program\n.\n\n\n\n\nThe Data Commons Architecture\n\uf0c1\n\n\n\n\nUser access to the Gen3 data commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.  \n\n\nOther secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.\n\n\n Diagram of the System Architecture \n\n\n\n\n\n\n\nContact Gen3\n\uf0c1\n\n\n\n\n\n\n\n\nLocations:\n\n\n\n\nShoreland\n:\n    5454 South Shore Drive\n    Suite 2B\n    Chicago, IL 60615\n\n\n\nUniversity of Chicago\n:\n    900 East 57th Street\n    10th Floor, Room 10148\n    Chicago, IL 60616\n\n\n\n\n\n\n\n\n\nEmail:\n\n\n\n\nGeneral Inquiries: \n\n\nTechnical Support:", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-gen3-documentation", 
            "text": "", 
            "title": "Welcome to Gen3 Documentation"
        }, 
        {
            "location": "/#the-data-commons-architecture", 
            "text": "User access to the Gen3 data commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.    Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.", 
            "title": "The Data Commons Architecture"
        }, 
        {
            "location": "/#contact-gen3", 
            "text": "Locations:   Shoreland :\n    5454 South Shore Drive\n    Suite 2B\n    Chicago, IL 60615  University of Chicago :\n    900 East 57th Street\n    10th Floor, Room 10148\n    Chicago, IL 60616     Email:   General Inquiries:   Technical Support:", 
            "title": "Contact Gen3"
        }, 
        {
            "location": "/user-guide/guide-overview/", 
            "text": "User Guide Overview\n\uf0c1\n\n\n\n\nThis documentation includes the following user guides for assisting data contributors with submitting data to a Gen3 data commons and assisting data analysts with accessing and analyzing data:\n\n\n\n\n\nHow to contribute a data project to a Gen3 Data Commons\n\n\nHow to access data in a Gen3 Data Commons\n\n\nHow to analyze data in the Jupyter Workspace\n\n\n\n\n\n\nAppendices\n\uf0c1\n\n\n\n\n\n\nUsing the CDIS data client\n\n\nWorking with the API\n\n\nWorking with the Proxy and Whitelist\n\n\nData Dictionary Viewer\n\n\nTemplate Metadata TSVs\n\n\nManaging Submission Timepoints", 
            "title": "Guide Overview"
        }, 
        {
            "location": "/user-guide/guide-overview/#user-guide-overview", 
            "text": "This documentation includes the following user guides for assisting data contributors with submitting data to a Gen3 data commons and assisting data analysts with accessing and analyzing data:   How to contribute a data project to a Gen3 Data Commons  How to access data in a Gen3 Data Commons  How to analyze data in the Jupyter Workspace", 
            "title": "User Guide Overview"
        }, 
        {
            "location": "/user-guide/guide-overview/#appendices", 
            "text": "Using the CDIS data client  Working with the API  Working with the Proxy and Whitelist  Data Dictionary Viewer  Template Metadata TSVs  Managing Submission Timepoints", 
            "title": "Appendices"
        }, 
        {
            "location": "/user-guide/data-contribution/", 
            "text": "Project Submission Overview\n\uf0c1\n\n\n\n\n\n\nSign relevant legal documents, fill out forms, and get credentials.\n\n\nPrepare and submit project metadata to the Windmill Data Portal.\n\n\nPrepare and submit data files to object storage.\n\n\n\n\nSteps to Contribute a Data Project to the Gen3 Commons\n\n\n\n\n\nReview and sign legal\n\n\nProvide Login Account and Project Name\n\n\nReview the data model\n\n\nPrepare metadata TSVs for each node in your project\n\n\nSubmit TSVs and validate metadata\n\n\nUpload data files to object storage\n\n\n\n\n* NOTE:  Gen3 members are encouraged to submit multiple projects to the commons. To do so, repeat steps 2-6 above.\n\n\n\n\n1. Review and sign legal agreements\n\uf0c1\n\n\n\n\nIn order to ensure the rights and interests of data contributors and study participants are protected, there are legal agreements that must be signed prior to data submission, data access, or data portal access. The data commons sponsor should confirm in writing with the authorizing party (usually the data commons operator) that access requests are valid.\n\n\nThe Gen3 data commons sponsor in collaboration with the commons operator should distribute the relevant documents to new users. Please review the referenced policies and sign and return the relevant legal agreements to \"support@datacommons.io\".\n\n\nCDIS recommends at least the following agreements be signed by the appropriate parties:\n\n Data Contributor Agreement - signed by persons submitting data\n\n Data Use/Access Agreement - signed by Principal Investigator (PI) requesting access for organization\n* Data Commons Services Agreement - signed by individuals using Windmill data portal services or downloading/analyzing data\n\n\nThese documents may also reference the following policies:\n\n Privacy and Security Agreement\n\n Intellectual Property Rights (IPR) Policy\n* Publication Policy\n\n\nNotes:\n\n If your organization will be both contributing and analyzing data, all three docs are required.\n\n If you only wish to contribute data, you do not need to sign the DUA.\n\n Windmill accounts are not to be shared by individuals. Each individual requesting data portal access needs to sign and return a DCSA, and the PI needs to also return a DUA for the organization.\n\n It is the user's responsibility to ensure that the relevant policies are reviewed before signing legal agreements.\n\n\n\n\n2. Provide Login Account and Project Name\n\uf0c1\n\n\n\n\nOnce legal documents are signed, the data commons operator will grant the individual data contributor appropriate permissions to login to and submit data to the Windmill data portal. Different permissions can be assigned at both the program and project level, and these permissions are associated with an individual's email address, which is used to login to the Windmill data portal.\n\n\n\n\n\n\nStep 1:\nThe Windmill data portal supports user authentication via \nOpenID Connect (OIDC; e.g., a gmail account)\n and/or \nNIH login\n (e.g., eRA commons). Please, send the account you wish to use for login to support@datacommons.io.\n\n\n\n\n\n\nStep 2:\nData contributors will also need to select an appropriate name for their project in the data portal. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you \nsubmit the metadata\n for a node in your project.\n\n\n\n\n\n\nProject name examples\n\n\n\nmycompanyname\n_P001\n\nmycompanyname\n_P002\n\nmycompanyname\n_ProjectID\n\n\n\n\nBreakdown:\n\n\n\n\n\n\"\n\" identifies the contributing organization\n\n\n\"P00x\" identifies the submission number for said organization\n\n\n\"ProjectID\" could also be used for a more descriptive identifier\n\n\n\n\n\n\nNOTE: Your project will have a prefix appended to it to identify the program. The \"program\" node exists in order to more finely tune permissions and, like the project node, is created by the commons operator. Thus, in the data portal, your project will be listed as \nProgram-ProjectName\n.\n\n\n\n\n\n\n3. Review the data model\n\uf0c1\n\n\n\n\nWhat is the data model?\n\n\n\nEvery Gen3 data commons employs a data model, which serves to describe and harmonize data sets, or organize data submitted by different contributors in a similar manner. Data harmonization facilitates cross-project analyses and is thus one of the pillars of the data commons paradigm.\n\n\nThe data model organizes experimental metadata variables, or \"properties\", into linked categories, or \"nodes\", through the use of a data dictionary. The data dictionary lists and describes all nodes in the data model, and it also defines and describes the properties (metadata variables) in each node.\n\n\nFor example, clinical variables like a primary cancer diagnosis or a subject's gender or race might go into the \"diagnosis\" or \"demographic\" nodes, respectively, while sample-related variables like how a tumor sample was collected and what analyte was extracted from it might go into the \"biospecimen\" or \"analyte\" nodes, respectively. Data files also have associated metadata variables like file size, format, and the file's location in object storage, and these properties are grouped into nodes that describe various types of data files, for example, \"mri_image\" for an MRI image data file.\n\n\nFinally, each node in the data dictionary is linked in a logical manner to other nodes, which facilitates generating a visual overview, or graphical model, of a project. The following image displays the data dictionary viewer, the 'biospecimen' node entry in the dictionary, and an example graphical model of a project in the \nBRAINCommons\n.\n\n\n\n\nWhy Do Gen3 Commons Use a Data Model?\n\n\n\n\n\nHaving all participating members use the same data model:\n\n\nAllows for standardized metadata elements across a commons.\n\n\nPermits flexible and scaleable API generation based on data commons software that reads the data model schema.   \n\n\nLets users query the commons API so that an ecosystem of applications can be built.\n\n\nHelps automate the validation of submitted data.   \n\n\n\n\n\n\n\n\nOnce you have access to the Windmill data submission portal, we recommend reviewing the commons' specific data dictionary by clicking \"Dictionary\" in the top navigation bar. Here you can determine which properties best describe your submission. This tool will help you understand the variable types, requirements, and node dependencies or links for your submission.\n\n\nIf you have an submission element that you believe isn't currently described in the model, notify the commons support team (support@datacommons.io) with a description of the data elements that you'd like to add, and they will make sure the sponsor or data modeling working group reviews your request and finds an appropriate home for your data elements.\n\n\n\n\n4. Prepare metadata TSVs for each node in your project\n\uf0c1\n\n\n\n\nData contributors will need to prepare metadata for their submission in tab-separated value (TSV) files for each node in their project.\n\n\nIt may be helpful to think of each TSV as a node in the graph of the data model. Column headers in the TSV are the properties (or metadata variables) stored in that node.  Each row is a \"record\" or \"entity\" in that node. Each record in every node will have a \"submitter_id\", which is a unique alphanumeric identifier for that record and is specified by the data submitter, and a \"type\", which is simply the node name.\n\n\nBesides the \"submitter_id\" and \"type\", which are required for every record, other properties are either required or not, and this can be determined in the data dictionary's \"Required\" column for a specific node.\n\n\nExample, blank TSV templates can be found \nhere\n and actual template TSVs for your commons are provided in each node's page in the data dictionary.\n\n\n\n\nDetermine Submission Order via Node Links\n\n\n\n\nThe prepared TSV files must be submitted in a specific order due to node links. Referring back to the graphical data model, you cannot submit a node without first submitting the nodes to which it is linked upstream. If you submit a metadata record out of order, that is, if you submit a record with a link to an upstream node that doesn't yet exist, the validator will reject the submission on the basis that the dependency you point to is not present with the error message \"INVALID_LINK\".\n\n\nThe \"program\" and \"project\" nodes are the most upstream nodes and are created by a commons administrator. So, the first node submitted by data contributor is usually the \"study\" or \"experiment\" node, which points directly upstream to the \"project\" node. Next, the study participants are recorded in the \"case\" node, and subsequently any clinical information (demographics, diagnoses, etc.), biospecimen data (biopsy samples, extracted analytes), etc., is linked to each case. Finally, metadata describing the actual raw data files to be uploaded to object storage are the last nodes submitted.\n\n\nSpecifying Required Links\n\n\n\n\n\nAt least one link is required for every record in a TSV, and sometimes multiple links should be specified. The links are specified in a TSV with the variable header \"\ns.submitter_id\", where \n is the upstream node a record is linking to. The value of this variable is the specific submitter_id of the link (the record in that upstream node which the current record is linked to).\n\n\nFor example, if there are two studies in a project, \"study-01\" and \"study-02\", the \"case.tsv\" TSV file will be uploaded to describe the study participants enrolled in each study. Each row in the \"case.tsv\" file would describe a single study participant, and the first case has the submitter_id \"case-01\".  There would be at least one link in that TSV specified with the column header \"studies.submitter_id\", and each row would have either \"study-01\" or \"study-02\" as the value for this column.\n\n\n\n\nSpecifying Multiple Links\n\n\n\n\nLinks can be one-to-one, many-to-one, one-to-many, and many-to-many. Since a single study participant can be enrolled in multiple studies, and a single study will have multiple cases enrolled in it, this link is \"many-to-many\". On the other hand, since a single study cannot be linked to multiple projects, but a single project can have many studies linked to it, the study -\n project link is \"many-to-one\".\n\n\nIn the above example, if \"case-01\" was enrolled in both \"study-01\" and \"study-02\", then there would be two columns to specify these links in the case.tsv file: \"studies.submitter_id#1\" and \"studies.submitter_id#2\". The values would be \"study-01\" for one of them and \"study-02\" for the other.\n\n\nOnce the \"case.tsv\" file is uploaded and creates the record \"case-01\" in the \"case\" node, if \"case-01\" had a diagnosis record linked to it, then in the \"diagnosis.tsv\" file to be uploaded next, there would be a column header \"cases.submitter_id\" and the value would be \"case-01\" (the case's \"submitter_id\") to link this diagnosis record to that case.\n\n\n\n\nRegister data files with the Windmill data portal\n\uf0c1\n\n\n\n\nSpecial attention must be given to \"data file\" nodes, which house variables that describe actual, raw data files that are to up be uploaded to object storage by the data contributor and later downloaded by data analysts. Specifically, data files must be \"registered\" in order to be downloadable using the Windmill data portal or the \ncdis-data-client\n.\n\n\nRegistration of data files simply means adding a column in the data file node's TSV named \"urls\" and entering the URL/address of each file in object storage (row in the TSV) in that column. This is usually a location in a project folder of a data commons bucket in s3 object storage, e.g.: \"s3://commons-bucket/project-name/filename\".\n\n\nFor example, say the following local files need to be registered and then uploaded:\n\n\ncommandline-prompt$ ls -l\n-rw-r--r--@  1 username  staff     6B May 30 15:18 file-1.dcm\n-rw-r--r--@  1 username  staff     7B May 30 15:18 file-2.dcm\n-rw-r--r--@  1 username  staff     8B May 30 15:18 file-3.dcm\n\n\n\n\nAdd a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,:\n\n\n\n\n\n\n\n\ntype\n\n\nsubmitter_id\n\n\nfilename\n\n\nfile_size\n\n\netc...\n\n\nurls\n\n\n\n\n\n\n\n\n\n\nmri_image\n\n\nfile-id-1\n\n\nfile-1.dcm\n\n\n6\n\n\n...\n\n\ns3://commons-bucket/project-name/file1.txt\n\n\n\n\n\n\nmri_image\n\n\nfile-id-2\n\n\nfile-2.dcm\n\n\n7\n\n\n...\n\n\ns3://commons-bucket/project-name/file2.txt\n\n\n\n\n\n\nmri_image\n\n\nfile-id-3\n\n\nfile-3.dcm\n\n\n8\n\n\n...\n\n\ns3://commons-bucket/project-name/file3.txt\n\n\n\n\n\n\n\n\nPlease make sure you check with the commons operator to make sure you have the correct commons bucket name prior to submitting a data file node TSV. Once the data files are registered, their metadata cannot be easily changed: the metadata record must be deleted and re-created.\n\n\nAlso be aware that metadata describing data files that will be uploaded to s3 object storage need to include the file size and md5sum in addition to the address of the file in s3 object storage. Therefore, before submitting data file metadata TSVs, make sure all of that information is included and correct so that data downloaders can confirm completeness of their download via the md5sum and file size.\n\n\n\n\n5. Submit TSVs and validate metadata\n\uf0c1\n\n\n\n\n Begin your metadata TSV submissions\n\n\n\nTo get you started submitting metadata TSVs, the first node, \"project\", has already been created for you by a commons administrator. Now, remembering that TSVs must be submitted for each node in a specific order, begin with the first node downstream of project, often \"study\" or \"experiment\" and continue to submit TSVs until all data file nodes are submitted and properly registered.\n\n\nFrom the Windmill data portal, click on \"Data Submission\" and then click \"Submit Data\" beside the project for which you wish to submit metadata TSVS.\n\n\nTo submit a TSV:\n1) Login to the Windmill data portal for your commons\n2) Click on \"Data Submission\" in the top navigation bar\n3) Click on \"Submit Data\" by the project for which you wish to submit metadata\n4) Click on \"Upload File\"\n5) Navigate to your TSV and click \"open\", the contents of the TSV should appear in the grey box below\n6) Click \"Submit\"\n\n\nNow you should see a message that indicates either success (green \"succeeded: 200\") or failure (grey \"failed: 400\"). Further details can be reviewed by clicking on \"DETAILS\", which displays the API response in JSON form. Each record/entity that was submitted (each row in the TSV) gets a true/false value for \"valid\" and lists \"errors\" if it was not valid.\n\n\nIf you see anything other than success, check the other fields for any information on what went wrong with the submission. The most descriptive information will be found in the individual entity transaction logs. Each line in the TSV will have its own output with the following attributes:\n\n\n{\n    \naction\n: \nupdate/create\n,\n    \nerrors\n: [\n        {\n            \nkeys\n: [\n                \nspecies (the property name)\n\n            ],\n            \nmessage\n: \n'Homo sapien' is not one of ['Drosophila melanogaster', 'Homo sapiens', 'Mus musculus', 'Mustela putorius furo', 'Rattus rattus', 'Sus scrofa']\n,\n            \ntype\n: \nERROR\n\n        }\n    ],\n    \nid\n: \n1d4e9bb0-515d-4158-b14b-770ab5077d8b (the UUID created for this record)\n,\n    \nrelated_cases\n: [],\n    \ntype\n: \ncase (the node name)\n,\n    \nunique_keys\n: [\n        {\n            \nproject_id\n: \ntraining (the project name)\n,\n            \nsubmitter_id\n: \ntraining-case-02 (the record/entity submitter_id)\n\n        }\n    ],\n    \nvalid\n: false,\n    \nwarnings\n: []\n}\n\n\n\n\nThe \"action\" above can be used to identify if the node was created new or just updated; when you resubmit, that is, submit to a node with the same submitter id, you will update the existing node. Other useful information includes the \"id\" for the record. This is the UUID for the record and is unique throughout the entirety of the data commons. The other \"unique_key\" provided is the tuple \"project_id\" and \"submitter_id\", which is to say the \"submitter_id\" combined with the \"project_id\" is a universal identifier for this record.\n\n\nTo confirm that a data file is properly registered, enter the UUID of a data file record in the index API endpoint of your data commons: usually \"\n/index/index/\n\", where \"data.commons-name.org\" is the URL of the Windmill data portal and \n is the specific UUID of a registered data file. You should see a JSON response that contains the url that was registered. If the record was not registered successfully, you will likely see an error message (you must be logged in or you will get an \"access denied\" type error).\n\n\n\n\n Troubleshooting and finishing your submission\n\n\n\n\nIf, against all odds, your TSV submission is perfect on the first try, you are finished with submission of that node, and you can move on to the next node. However, if the submission throws errors or claims your submission to be invalid, you will need to fix your submission.\n\n\nThe best first step is to go through the outputs from the individual entities. In the errors field will be a rough description of what failed the validation check. The most common problems are simple issues such as spelling errors, mislabeled properties, or missing required fields.\n\n\n\n\nProvide feedback\n\n\n\n\nPlease contact the support team to let us know when your submission is complete.\n\n\nYou may receive errors for what you think is a valid submission. If you feel what you have provided for a particular entity is valid, please contact the commons support team at support@datacommons.io. We will be happy to accommodate any necessary changes. We can always add new nodes, properties, or values.\n\n\n\n\nHow can I learn more about my existing submission?\n\n\n\n\nWhen you are viewing a project, you can click on a node name to view the records in that node. From here you can download, view, or completely delete records associated with any project you have delete access to.\n\n\n\n\n6. Upload data files to object storage\n\uf0c1\n\n\n\n\n Preparing your data \n\n\n\nData files such as sequencing data (BAM, FASTQ), assay results, images, PDFs, etc., should be uploaded with the CDIS data client.\nFor detailed instructions, visit \nthe cdis-data client documentation\n. The metadata TSVs you prepared do not need to be submitted to the object store, as they have already been submitted via the API.\n\n\n\n\nDownloaded the \ncompiled binary\n for your operating system.\n\n\nConfigure a profile with credentials: \n./cdis-data-client configure --profile \nprofile\n --cred \ncredentials.json\n\n\nUpload a data file using its UUID: \n./cdis-data-client upload --profile \nprofile\n --uuid \nUUID\n --file=\nfilename", 
            "title": "Data Contribution"
        }, 
        {
            "location": "/user-guide/data-contribution/#project-submission-overview", 
            "text": "Sign relevant legal documents, fill out forms, and get credentials.  Prepare and submit project metadata to the Windmill Data Portal.  Prepare and submit data files to object storage.", 
            "title": "Project Submission Overview"
        }, 
        {
            "location": "/user-guide/data-contribution/#1-review-and-sign-legal-agreements", 
            "text": "In order to ensure the rights and interests of data contributors and study participants are protected, there are legal agreements that must be signed prior to data submission, data access, or data portal access. The data commons sponsor should confirm in writing with the authorizing party (usually the data commons operator) that access requests are valid.  The Gen3 data commons sponsor in collaboration with the commons operator should distribute the relevant documents to new users. Please review the referenced policies and sign and return the relevant legal agreements to \"support@datacommons.io\".  CDIS recommends at least the following agreements be signed by the appropriate parties:  Data Contributor Agreement - signed by persons submitting data  Data Use/Access Agreement - signed by Principal Investigator (PI) requesting access for organization\n* Data Commons Services Agreement - signed by individuals using Windmill data portal services or downloading/analyzing data  These documents may also reference the following policies:  Privacy and Security Agreement  Intellectual Property Rights (IPR) Policy\n* Publication Policy  Notes:  If your organization will be both contributing and analyzing data, all three docs are required.  If you only wish to contribute data, you do not need to sign the DUA.  Windmill accounts are not to be shared by individuals. Each individual requesting data portal access needs to sign and return a DCSA, and the PI needs to also return a DUA for the organization.  It is the user's responsibility to ensure that the relevant policies are reviewed before signing legal agreements.", 
            "title": "1. Review and sign legal agreements"
        }, 
        {
            "location": "/user-guide/data-contribution/#2-provide-login-account-and-project-name", 
            "text": "Once legal documents are signed, the data commons operator will grant the individual data contributor appropriate permissions to login to and submit data to the Windmill data portal. Different permissions can be assigned at both the program and project level, and these permissions are associated with an individual's email address, which is used to login to the Windmill data portal.    Step 1:\nThe Windmill data portal supports user authentication via  OpenID Connect (OIDC; e.g., a gmail account)  and/or  NIH login  (e.g., eRA commons). Please, send the account you wish to use for login to support@datacommons.io.    Step 2:\nData contributors will also need to select an appropriate name for their project in the data portal. The project name will be used to create the project node from which you can build out the rest of your submission and is an essential identifier. For example, the project name will need to be provided when you  submit the metadata  for a node in your project.", 
            "title": "2. Provide Login Account and Project Name"
        }, 
        {
            "location": "/user-guide/data-contribution/#3-review-the-data-model", 
            "text": "", 
            "title": "3. Review the data model"
        }, 
        {
            "location": "/user-guide/data-contribution/#4-prepare-metadata-tsvs-for-each-node-in-your-project", 
            "text": "Data contributors will need to prepare metadata for their submission in tab-separated value (TSV) files for each node in their project.  It may be helpful to think of each TSV as a node in the graph of the data model. Column headers in the TSV are the properties (or metadata variables) stored in that node.  Each row is a \"record\" or \"entity\" in that node. Each record in every node will have a \"submitter_id\", which is a unique alphanumeric identifier for that record and is specified by the data submitter, and a \"type\", which is simply the node name.  Besides the \"submitter_id\" and \"type\", which are required for every record, other properties are either required or not, and this can be determined in the data dictionary's \"Required\" column for a specific node.  Example, blank TSV templates can be found  here  and actual template TSVs for your commons are provided in each node's page in the data dictionary.", 
            "title": "4. Prepare metadata TSVs for each node in your project"
        }, 
        {
            "location": "/user-guide/data-contribution/#register-data-files-with-the-windmill-data-portal", 
            "text": "Special attention must be given to \"data file\" nodes, which house variables that describe actual, raw data files that are to up be uploaded to object storage by the data contributor and later downloaded by data analysts. Specifically, data files must be \"registered\" in order to be downloadable using the Windmill data portal or the  cdis-data-client .  Registration of data files simply means adding a column in the data file node's TSV named \"urls\" and entering the URL/address of each file in object storage (row in the TSV) in that column. This is usually a location in a project folder of a data commons bucket in s3 object storage, e.g.: \"s3://commons-bucket/project-name/filename\".  For example, say the following local files need to be registered and then uploaded:  commandline-prompt$ ls -l\n-rw-r--r--@  1 username  staff     6B May 30 15:18 file-1.dcm\n-rw-r--r--@  1 username  staff     7B May 30 15:18 file-2.dcm\n-rw-r--r--@  1 username  staff     8B May 30 15:18 file-3.dcm  Add a column 'urls' to the TSV and entering the full s3 path for each file in that column, e.g.,:     type  submitter_id  filename  file_size  etc...  urls      mri_image  file-id-1  file-1.dcm  6  ...  s3://commons-bucket/project-name/file1.txt    mri_image  file-id-2  file-2.dcm  7  ...  s3://commons-bucket/project-name/file2.txt    mri_image  file-id-3  file-3.dcm  8  ...  s3://commons-bucket/project-name/file3.txt     Please make sure you check with the commons operator to make sure you have the correct commons bucket name prior to submitting a data file node TSV. Once the data files are registered, their metadata cannot be easily changed: the metadata record must be deleted and re-created.  Also be aware that metadata describing data files that will be uploaded to s3 object storage need to include the file size and md5sum in addition to the address of the file in s3 object storage. Therefore, before submitting data file metadata TSVs, make sure all of that information is included and correct so that data downloaders can confirm completeness of their download via the md5sum and file size.", 
            "title": "Register data files with the Windmill data portal"
        }, 
        {
            "location": "/user-guide/data-contribution/#5-submit-tsvs-and-validate-metadata", 
            "text": "", 
            "title": "5. Submit TSVs and validate metadata"
        }, 
        {
            "location": "/user-guide/data-contribution/#6-upload-data-files-to-object-storage", 
            "text": "", 
            "title": "6. Upload data files to object storage"
        }, 
        {
            "location": "/user-guide/data-access/", 
            "text": "Data Access Overview \n\n\n\n\n\nThe sponsor of a Gen3 data commons typically decides how users will access data in object storage. In some cases, approved users may be allowed to download data directly to their local computer from within an internet browser or with the \ncdis-data-client\n. When more security is required, users may be required to download and analyze data in a protected environment, such as a virtual machine (VM) in a virtual private cloud (VPC).\n\n\n\n\nAccessing data from within a browser\n\uf0c1\n\n\n\n\nOnce data files are \nregistered\n, their address in s3 object storage can be obtained by providing the file's UUID to the following URL:\nhttps://data.gen3.org/index/index/UUID\n\n\nData files can be downloaded by providing the file's UUID to the following URL:\nhttps://data.gen3.org/user/data/download/UUID\n\n\n\n\nDownloading data with the cdis-data-client\n\uf0c1\n\n\n\n\nData files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files.\n\n\nDownload the latest release of the client here.\n\n\nOnce downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal:\n\n\n./cdis-data-client configure --profile \nprofile_name\n --cred /path/to/api/credentials.json\n\n\n\n\nThe client will then prompt you for the API. Enter the API of your commons, e.g.:\n\n\nAPI endpoint: https://gen3.datacommons.io/\n\n\n\n\nTo download a data file, pass the file's UUID to the client:\n\n\ncdis-data-client download --profile \nprofile_name\n --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014\n\n\n\n\nIn the above command, \ndownload\n mode is specified, the \nprofile_name\n we configured with the API credentails earlier is used, and a filename (\nfilename.tsv\n) was specified for our local copy of the downloaded file.\n\n\n\n\nAccessing data from the Virtual Private Cloud\n\uf0c1\n\n\n\n\nIf additional security is required, the cdis-data-client can be installed on a VM in the VPC to download files. The following instructions detail how to login to a VM.\n\n\n1. Send credentials and get welcome email\n\uf0c1\n\n\n\n\nSend public SSH Key and OpenConnectID account to Gen3 commons team.\n\n\nTo access the Virtual Private Cloud (VPC), users will need to send their public ssh key (or \"pubkey\") and an email that supports Oauth (often gmail) to \n.   \n\n\n\n\nNOTE:   Do not send your private ssh key.   This is confidential and should never be shared with anyone.  \n\n\n\n\n I'm not familiar with SSH - how do I generate a keypair? \n\n\n\nGithub has a very nice \nssh tutorial\n with step-by-step instructions for Mac, Windows (users with gitbash installed), and Linux users.   If you're new to using SSH we'd recommend reviewing the links:\n\n\n\n\nAbout SSH\n\n\nChecking for Existing SSH Keys\n\n\nGenerating a new SSH key and adding it to the SSH Agent\n\n\n\n\n\n\nNOTE:  For Windows users, we recommend installing \nGit for Windows\n and using the Git Bash feature to interact on the command line, manage ssh keys, and s3 credentials.   \n\n\n\n\nReceive a welcome email\n\n\n\nGen3 users with the appropriate signed legal documents will be sent an email that gives the following unique information:\n\n\n\n\nusername (this is used to ssh to the VPC login node) - eg:  \nssh -A \nusername\n@\nlogin-ip-address\n\n\nan IP associated with your new VM\n\n\n\n\n\n\n2. SSH to Virtual Machine: config\n\uf0c1\n\n\n\n\nHow will I access the Login Node and my Virtual Machine?\n\n\n\nGen3 Commons users may login to the Virtual Private Cloud (VPC) headnode, then hop over to an analysis virtual machine (VM).   For more information on the \nVPC architecture\n.\n\n\nIn your \nwelcome email\n you received your username and your vm.   In order to access your VM, you first must access the VPC login node.   This configuration helps ensure the security of the Gen3 commons by having your VM in a private subnet.   Using the credentials from your welcome email this can be done in the following order:\n\n\n\n\nSSH to login node:   \nssh -A \nusername\n@3\nlogin-node-IP\n\n\nSSH from login node to your VM:  \nssh -A ubuntu@\nmy_VM_IP\n\n\n\n\n\n\nNOTE:  the \n-A\n flag forwards all the keys in your key chain.   For more details on managing SSH keys, check the guides linked in the \nprevious step\n.\n\n\nNOTE 2:   You can't login to your analysis VM (in the private subnet) without first logging in to the login node (in the public subnet).  \n\n\n\n\nAdvanced users can manage these connections however they see fit. For other users, we recommend updating your SSH config file so you can setup a 'multihop' ssh tunnel. To 'multihop' in this context is to use a single command to get to your VM. What follows are instructions for updating your \n.ssh/config\n file to get to your VM in a single command.\n\n\n\n\n3. Setting up an ssh config for 'multihop'\n\uf0c1\n\n\n\n\nTo start, go to your .ssh directory in your laptop home directory.\n\n\ncd ~/.ssh\n\n\n\n\nIf this directory does not exist, make one.\n\n\nmkdir -p ~/.ssh\ncd ~/.ssh\n\n\n\n\nWithin this directory create a file named \"config\" [Note: I use vim here but any text editor will do]:\n\n\nvim config\n\n\n\n\nIn this file you can specify various hosts for access via ssh. Your host for the head login node should look something like this:\n\n\nHost login-node\n    Hostname \nip.address.of.login.node\n\n    User \nYOUR_USERNAME\n\n    IdentityFile \n/path/to/YOUR_CREDFILE\n\n    ForwardAgent yes\n\n\n\n\nWhere /path/to/YOUR_CREDFILE might be, e.g., \n~/.ssh/id_rsa\n\n\nThe username will be provided to you by the Gen3 support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the Gen3 head-node using this host:\n\n\nssh login-node\n\n\n\n\nExit the head login node and return to your local machine. Back in your config file, add a new host for your analysis VM:\n\n\nHost analysis\n    Hostname \nYOUR_VM_IP\n\n    User ubuntu\n    ProxyCommand ssh -q -AXY login-node -W %h:%p\n\n\n\n\nOnce again you will receive the Hostname IP from the Gen3 support team in step 4.  This host will route you through the head login node and take you directly to your personal Gen3 analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM:\n\n\nssh analysis\n\n\n\n\nIf you've done everything correctly, you should now be in the analysis VM.  \n\n\n Ready to work! \n\n\n\nYou're ready to use whatever tools you wish to analyze data in the commons within your VM.   For requests for alternative configurations, analysis storage, or other needs please contact \n.\n\n\nFor an example of how you could use a Jupyter Notebook to run analysis in the browser on your local computer, please continue on to the \nnext section\n.    There are lots of good examples that may be useful to you.", 
            "title": "Data Access"
        }, 
        {
            "location": "/user-guide/data-access/#accessing-data-from-within-a-browser", 
            "text": "Once data files are  registered , their address in s3 object storage can be obtained by providing the file's UUID to the following URL:\nhttps://data.gen3.org/index/index/UUID  Data files can be downloaded by providing the file's UUID to the following URL:\nhttps://data.gen3.org/user/data/download/UUID", 
            "title": "Accessing data from within a browser"
        }, 
        {
            "location": "/user-guide/data-access/#downloading-data-with-the-cdis-data-client", 
            "text": "Data files can also be downloaded using the \"cdis-data-client\", which provides a simple command-line interface for downloading and uploading data files.  Download the latest release of the client here.  Once downloaded and installed, the client can be configured with the API credentials.json downloaded from your Profile in the data portal:  ./cdis-data-client configure --profile  profile_name  --cred /path/to/api/credentials.json  The client will then prompt you for the API. Enter the API of your commons, e.g.:  API endpoint: https://gen3.datacommons.io/  To download a data file, pass the file's UUID to the client:  cdis-data-client download --profile  profile_name  --file ./filename.tsv --uuid d7a5XXXX-XXXX-XXXX-XXXX-XXXX53583014  In the above command,  download  mode is specified, the  profile_name  we configured with the API credentails earlier is used, and a filename ( filename.tsv ) was specified for our local copy of the downloaded file.", 
            "title": "Downloading data with the cdis-data-client"
        }, 
        {
            "location": "/user-guide/data-access/#accessing-data-from-the-virtual-private-cloud", 
            "text": "If additional security is required, the cdis-data-client can be installed on a VM in the VPC to download files. The following instructions detail how to login to a VM.", 
            "title": "Accessing data from the Virtual Private Cloud"
        }, 
        {
            "location": "/user-guide/data-access/#1-send-credentials-and-get-welcome-email", 
            "text": "", 
            "title": "1. Send credentials and get welcome email"
        }, 
        {
            "location": "/user-guide/data-access/#2-ssh-to-virtual-machine-config", 
            "text": "", 
            "title": "2. SSH to Virtual Machine: config"
        }, 
        {
            "location": "/user-guide/data-access/#3-setting-up-an-ssh-config-for-multihop", 
            "text": "To start, go to your .ssh directory in your laptop home directory.  cd ~/.ssh  If this directory does not exist, make one.  mkdir -p ~/.ssh\ncd ~/.ssh  Within this directory create a file named \"config\" [Note: I use vim here but any text editor will do]:  vim config  In this file you can specify various hosts for access via ssh. Your host for the head login node should look something like this:  Host login-node\n    Hostname  ip.address.of.login.node \n    User  YOUR_USERNAME \n    IdentityFile  /path/to/YOUR_CREDFILE \n    ForwardAgent yes  Where /path/to/YOUR_CREDFILE might be, e.g.,  ~/.ssh/id_rsa  The username will be provided to you by the Gen3 support team and will be tied to the credential file that you provide us when setting up the account. Save this file and exit. Back in the command line terminal you should now be able to log in to the Gen3 head-node using this host:  ssh login-node  Exit the head login node and return to your local machine. Back in your config file, add a new host for your analysis VM:  Host analysis\n    Hostname  YOUR_VM_IP \n    User ubuntu\n    ProxyCommand ssh -q -AXY login-node -W %h:%p  Once again you will receive the Hostname IP from the Gen3 support team in step 4.  This host will route you through the head login node and take you directly to your personal Gen3 analysis VM. Once again save the file and exit. In the terminal, try and log in to the submission VM:  ssh analysis  If you've done everything correctly, you should now be in the analysis VM.", 
            "title": "3. Setting up an ssh config for 'multihop'"
        }, 
        {
            "location": "/user-guide/data-analysis/", 
            "text": "Data Analysis in the Gen3 Commons \n\n\n\nHow data is accessed in a Gen3 Data Commons must be agreed upon by the sponsor(s), data contributor(s), and the operator(s). Some data commons have rules that data cannot be downloaded outside of a Virtual Private Cloud. In these cases, data analysts may need to access and configure a virtual machine (VM) in the VPC where all analyses will be done. Other data commons may be able to grant users permissions to download data files directly to their local computers.\n\n\n\n\nJupyterHub: Using Windmill's Workspace\n\uf0c1\n\n\n\n\nThe Windmill data portal may provide a Workspace where users can access a personalized Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal.\n\n\n\nClick \"Start My Server\" to start the Jupyter server in your Workspace:\n\n\n\nOr if a server is already running, click on \"My Server\" to access your files.\n\n\n\nThe Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files.\n\n\nAfter editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page.\n\n\n\nThe following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\":\n\n\n\nThis clip demonstrates creating a new Jupyter notebook in the R language:\n\n\n\nTerminal sessions can also be started in the Workspace and used to download other tools.\n\n\n\nYou can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them.\n\n\n\n\n\nRunning a Jupyter server on a virtual machine (VM)\n\uf0c1\n\n\n\n\nThe bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses.    Both can be found in your VM in the analysis folder.    They can also be found at: \nhttps://github.com/occ-data/gen3-functions\n.    The Gen3 community is encouraged to add to the functions library or improve the notebook.  \n\n\n\n\nNOTE:   As the Gen3 community updates repositories, you can keep them up to date using \ngit pull origin master\n in the \nfunctions\n folder.   It has already been initialized to sync with this repository.\n\n\nNOTE2: If you receive an error when trying to do \ngit pull\n, you may need to set proxies and/or either save or drop any changes you've made:\n\n\n\n\n# set proxies:\nexport http_proxy=\nhttp://cloud-proxy.internal.io:3128\n\nexport https_proxy=\nhttp://cloud-proxy.internal.io:3128\n\n\n# to drop changes:\ngit stash save --keep-index\ngit stash drop\n\n# or save changes\ngit commit .\n\n# Update CDIS utils python libraries:\ngit clone https://github.com/uc-cdis/cdis-python-utils.git\ncd cdis-python-utils\nsudo -E python setup.py install\n\n# unset proxies to get juypter notebook to work again\nunset http_proxy;\nunset https_proxy;\n\n\n\n\nWhat follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser.   In the notebook, you'll learn about basic Gen3 commons operations like:  \n\n\n\n\nQuerying the API for metadata associated with submissions\n\n\nPulling data into your VM for analysis\n\n\nRunning a simple analysis over a file and collection of files\n\n\nPlotting the results\n\n\n\n\nIn the Jupyter notebook, many of the calls rely on more complex functions described in the \nanalysis_functions file\n.   It is worth taking the time to understand how this file works so you can use and customize it or build your own tools.    \n\n\nWe would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile.   Just contact \n for more information.\n\n\nRunning the notebook in your VM\n\uf0c1\n\n\nAfter we're logged in to our analysis VM and in the functions directory (from home: \ncd functions\n), run the jupyter notebook server.  \n\n\nRun the notebook server:\n\n\njupyter notebook --no-browser --port=8889\n\n\n\n\n\n\nNOTE:   You can stop a Juptyer server at anytime via \nctrl + c\n\n\n\n\nPort forwarding to your VM\n\uf0c1\n\n\nNext you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine.   \n\n\nOn a terminal session from your local machine (not in the VM) setup the connection:\n\n\nssh -N -L localhost:8888:localhost:8889 analysis\n\n\n\n\n\n\nNOTE:   In the example above \"analysis\" is the name of the ssh shortcut we \nsetup back in step 2\n.\n\n\n\n\nAccess the notebook in via browser\n\uf0c1\n\n\nIn your preferred browser and enter http://localhost:8888/;   Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.\n\n\n Example:   Run Server, port forward, access notebook in browser\n\n\n\n\n\nReview and follow the notebook\n\uf0c1\n\n\n Credentials \n\n\n\nThe notebook makes use of both a \n.secrets file\n that came preloaded in your VM in the home directory that lets you query the metadata API from the VM, and the \ns3 profile you created\n to pull 'raw' data into your VM for analysis.\n\n\nThe first thing you'll want to do is update the \"profile\" variable in the first cell to whatever the name of your s3 profile is.       \n\n\n\n\nNOTE:  If you have forgotten what you called your profile, you can always take a look at the credential file to review.  From the VM run:  \nvi ~/.aws/credentials\n.  \n\n\n\n\n Jupyter Basics \n\n\n\nIf you're not familiar with Jupyter notebooks, you have a few options to run the commands inside.   You can review line by line (select cell - \nShift+Enter\n) or you can run all from the \"Kernel\" menu at the top of the browser.   \n\n\n Shutting Down your Server\n\n\n\nWhen you're done working, we encourage you to shut down your Jupyter server via \nctrl + c\n in the VM that's running it.  You don't have to do this every time, but you should do it when you don't need it any more.   \n\n\n VM Termination \n\n\n\nAt this point in the Gen3 commons development, you should contact \n when you no longer need your VM active.   Active VMs accrue hourly charges (currently paid for by the Consortium and grants), so it's important to not waste valuable resources.", 
            "title": "Data Analysis"
        }, 
        {
            "location": "/user-guide/data-analysis/#jupyterhub-using-windmills-workspace", 
            "text": "The Windmill data portal may provide a Workspace where users can access a personalized Jupyter server for data exploration and analysis. To access the workspace, click \"Workspace\" in the top navigation bar of the data portal.  Click \"Start My Server\" to start the Jupyter server in your Workspace:  Or if a server is already running, click on \"My Server\" to access your files.  The Jupyter Workspace supports interactive programming sessions in the Python and R languages. Code blocks are entered in cells, which can be executed individually in any order or all at once. Code documentation and comments can also be entered in cells, and the cell type can be set to support, e.g., Markdown. Results, including plots, tables, and graphics, can be generated in the workspace and downloaded as files.  After editing a Jupyter notebook, it can be saved in the Workspace to revisit later by clicking the \"save\" icon or \"File\" and then \"Save and checkpoint\". Notebooks and files can also be downloaded from the server to your local computer by clicking \"File\" then \"Download as\". Similarly, notebooks and files can be uploaded to the Jupyter server from a local computer by clicking on the \"upload\" button from the server's home page.  The following clip illustrates downloading the credentials.json from the \"Identity\" page in the data portal, then uploading that file to the Jupyter Workspace and reading it in a Python notebook named \"Gen3_authentication.ipynb\":  This clip demonstrates creating a new Jupyter notebook in the R language:  Terminal sessions can also be started in the Workspace and used to download other tools.  You can manage active Notebook and terminal processes by clicking on \"Running\". Clicking \"shutdown\" will terminate the terminal session or close the Jupyter notebook. Be sure to save your notebooks before terminating them.", 
            "title": "JupyterHub: Using Windmill's Workspace"
        }, 
        {
            "location": "/user-guide/data-analysis/#running-a-jupyter-server-on-a-virtual-machine-vm", 
            "text": "The bioinformatics team at the Center for Data Intensive Science (CDIS) at University of Chicago has put together a basic python library and a sample analysis notebook to help jumpstart commons analyses.    Both can be found in your VM in the analysis folder.    They can also be found at:  https://github.com/occ-data/gen3-functions .    The Gen3 community is encouraged to add to the functions library or improve the notebook.     NOTE:   As the Gen3 community updates repositories, you can keep them up to date using  git pull origin master  in the  functions  folder.   It has already been initialized to sync with this repository.  NOTE2: If you receive an error when trying to do  git pull , you may need to set proxies and/or either save or drop any changes you've made:   # set proxies:\nexport http_proxy= http://cloud-proxy.internal.io:3128 \nexport https_proxy= http://cloud-proxy.internal.io:3128 \n\n# to drop changes:\ngit stash save --keep-index\ngit stash drop\n\n# or save changes\ngit commit .\n\n# Update CDIS utils python libraries:\ngit clone https://github.com/uc-cdis/cdis-python-utils.git\ncd cdis-python-utils\nsudo -E python setup.py install\n\n# unset proxies to get juypter notebook to work again\nunset http_proxy;\nunset https_proxy;  What follows in this wiki is a guide to setting up this Jupyter notebook so that you can run everything in your browser.   In the notebook, you'll learn about basic Gen3 commons operations like:     Querying the API for metadata associated with submissions  Pulling data into your VM for analysis  Running a simple analysis over a file and collection of files  Plotting the results   In the Jupyter notebook, many of the calls rely on more complex functions described in the  analysis_functions file .   It is worth taking the time to understand how this file works so you can use and customize it or build your own tools.      We would gladly publish and share any tools, Docker images, function libraries, notebooks, etc via Github or via this Sage Synapse profile.   Just contact   for more information.", 
            "title": "Running a Jupyter server on a virtual machine (VM)"
        }, 
        {
            "location": "/user-guide/data-analysis/#running-the-notebook-in-your-vm", 
            "text": "After we're logged in to our analysis VM and in the functions directory (from home:  cd functions ), run the jupyter notebook server.    Run the notebook server:  jupyter notebook --no-browser --port=8889   NOTE:   You can stop a Juptyer server at anytime via  ctrl + c", 
            "title": "Running the notebook in your VM"
        }, 
        {
            "location": "/user-guide/data-analysis/#port-forwarding-to-your-vm", 
            "text": "Next you'll want to set up a connection so that you can access the notebook being served from the VM to a browser in your local machine.     On a terminal session from your local machine (not in the VM) setup the connection:  ssh -N -L localhost:8888:localhost:8889 analysis   NOTE:   In the example above \"analysis\" is the name of the ssh shortcut we  setup back in step 2 .", 
            "title": "Port forwarding to your VM"
        }, 
        {
            "location": "/user-guide/data-analysis/#access-the-notebook-in-via-browser", 
            "text": "In your preferred browser and enter http://localhost:8888/;   Then from the VM terminal session, copy and paste the token from the notebook server into the requested spot in your browser.", 
            "title": "Access the notebook in via browser"
        }, 
        {
            "location": "/user-guide/data-analysis/#review-and-follow-the-notebook", 
            "text": "", 
            "title": "Review and follow the notebook"
        }, 
        {
            "location": "/appendices/architecture/", 
            "text": "Gen3 Data Commons Architecture\n\uf0c1\n\n\n\n\nUser access to the Gen3 Commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.  \n\n\nOther secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.\n\n\nDiagram of the Gen3 System Architecture", 
            "title": "Data Commons Architecture"
        }, 
        {
            "location": "/appendices/architecture/#gen3-data-commons-architecture", 
            "text": "User access to the Gen3 Commons runs through a VPC or Virtual Private Cloud.   Access to data and analysis tools through a Virtual Private Cloud (VPC) allows for balance of usability and security.   All access is through a monitored head node.  Data is not directly accessed from the Internet.    Other secure and compliant Gen3 member systems (including cloud based systems) can access Gen3 data through the API.", 
            "title": "Gen3 Data Commons Architecture"
        }, 
        {
            "location": "/appendices/data-dictionary/", 
            "text": "Data Dictionary Viewer\n\uf0c1\n\n\n\n\n\n\n\n\nThe Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.   \n\n\n\n\n\n\nIn addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.  \n\n\n\n\n\n\nGen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at: \nhttps://data.Gen3.org/dd/\n\n\n\n\n\n\n\n\n\n\nViewing data dictionary as a graph:\n\uf0c1\n\n\n\n\nToggling the graph view:\n\uf0c1\n\n\n\n\nViewing data dictionary as tables:\n\uf0c1\n\n\n\n\nToggling between different views\n\uf0c1", 
            "title": "Data Dictionary"
        }, 
        {
            "location": "/appendices/data-dictionary/#data-dictionary-viewer", 
            "text": "The Data Dictionary Viewer is designed to make it easier to understand the data model, the field types associated with each node, and the potential values associated with each field. It helps you understand the available fields in a node and review the dependencies a given node has to the existence of a prior node.  This is an invaluable tool for both the submission of data and later analysis of the entire commons.       In addition to drilling down on the properties of each node, the Data Dictionary Viewer will also let you toggle views and browse the nodes as a graph and as tables.      Gen3 members can use it through the 'dictionary' icon at data.Gen3.org or directly at:  https://data.Gen3.org/dd/", 
            "title": "Data Dictionary Viewer"
        }, 
        {
            "location": "/appendices/data-dictionary/#viewing-data-dictionary-as-a-graph", 
            "text": "", 
            "title": "Viewing data dictionary as a graph:"
        }, 
        {
            "location": "/appendices/data-dictionary/#toggling-the-graph-view", 
            "text": "", 
            "title": "Toggling the graph view:"
        }, 
        {
            "location": "/appendices/data-dictionary/#viewing-data-dictionary-as-tables", 
            "text": "", 
            "title": "Viewing data dictionary as tables:"
        }, 
        {
            "location": "/appendices/data-dictionary/#toggling-between-different-views", 
            "text": "", 
            "title": "Toggling between different views"
        }, 
        {
            "location": "/appendices/api-gen3/", 
            "text": "Working with the API\n\uf0c1\n\n\n\n\nWhat does the API do?\n\uf0c1\n\n\n\n\nThe API is created programmatically based on the \nGen3 commons data model\n.   All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps \n4-6 in the Data Contribution section\n).   \n\n\nWith the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons.   The API can be queried programmatically or through provided tools like the submission portal.  \n\n\nWe use \nGraphQL\n to manage the metadata in the Gen3 commons.  To learn the basics of writing queries in Graph QL, we recommend \nthis introduction\n.\n\n\n\n\nWhat's an example of the API at work?\n\uf0c1\n\n\n\n\nThe Gen3 commons team has created a few matrices to help describe submitted data.   These are linked to from: \nhttps://www.gen3.org/data-group/\n\n\nThese query the API for the desired metadata and return the matrices.   They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear.   If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.   \n\n\n\n\nCredentials to query the API\n\uf0c1\n\n\n\n\nLike your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you programmatically query or submit data to the API. This credential is used every time you make an API call.\n\n\nEach API key is valid for a month and is used to receive a temporary access token that is valid for only 30 minutes. The access token is what must be sent to the Gen3 API to access data in the commons.\n\n\nFor users granted data access, the API key is provided on your Profile page when you click \"Create API key\".\n\n\n\n\n\n\n\n\nWhile displayed, you can click \"copy\" to copy the API key to your clipboard or \"download\" to download a \"credentials.json\" file containing the id/key pair in json format.\n\n\n\n\n\n\n\n\nIn python, the following command is sent, using the module \"requests\", to receive the access token:\n\n\n# Save your copied credentials.json from the website into a variable \nkey\n:\nkey =  { \napi_key\n: \nactual-key\n, \nkey_id\n: \na-key-uuid\n }\n\n# Import the \nrequests\n python module:\nimport requests\n\n# Pass the API key to the Gen3 API using \nrequests.post\n to receive the access token:\ntoken = requests.post('https://gen3.commons.io/user/credentials/cdis/access_token', json=key).json()\n\n# Now you should see your access_token displayed when you enter:\ntoken\n\n\n\n\n\nWhen submitting a graphQL query to the Gen3 API, or requesting data download/upload, include the access token in your request header:\n\n\nheaders = {'Authorization': 'bearer '+ token['access_token']}\n\n# a graphQL endpoint query using the \nkey\n json:\nquery = {'query':\n{project(first:0){project_id id}}\n};\nql = requests.post('https://gen3.commons.io/api/v0/submission/graphql/', json=query, headers=headers)\nprint(ql.text) # display the response\n\n# Data download via API endpoint request:\ndurl = 'https://gen3.commons.io/api/v0/submission/\nprogram\n/\nproject\n/export?format=tsv\nids=' + ids[0:-1] # define the download url with the UUIDs of the records to download in \nids\n list\ndl = requests.get(durl, headers=headers)\nprint(dl.text) # display response\n\n# Data upload via API endpoint request:\nheaders['content-type']='text/tab-separated-values' # add the content-type to header\nu = requests.put('https://gen3.commons.io/api/v0/submission/project-id', data=tsv, headers=headers)\n\n\n\n\nIf you receive an error like \"You don't have access... \", then you will most likely need to update your API key or request a new access token.\n\n\n\n\nQueries in the submission portal:   graphQL\n\uf0c1\n\n\n\n\nYou can run queries directly in the submission portal by clicking the \"Query\" magnifying glass or directly at: \nhttps://data.gen3.org/graphql\n. Queries are essential as you begin analyses. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need.\n\n\n\n\nPagination and Offsets:   Default = first 10 entries\n\uf0c1\n\n\nQueries by defult return the first 10 entries. If you want more than that, you can specify it in the query call: \n(first:1000)\n\n\nIn the case that too many results are returned, you may receive a timeout error. In that case, you may want to use \npagination\n. For example, if there are 2,550 records returned, and your graphiQL query is timing out with \n(first:3000)\n, then break your query into multiple queries with offsets:\n\n\n(first:1000, offset:0)      # this will return records 0-1000\n(first:1000, offset:1000)   # this will return records 1000-2000\n(first:1000, offset:2000)   # this will return records 2000-2,550\n\n\n\n\nUpdating the example template \ndetails from experiment\n sample query to call the first 1000, the call becomes:  \n\n\n{\n    \nquery\n:\n query Test {\n        experiment (first:1000, submitter_id: \nINSERT submitter_id\n) {  \n            experimental_intent\n            experimental_description\n            number_samples_per_experimental_group\n            type_of_sample\n            type_of_specimen\n        }\n    } \n\n}\n\n\n\n\n\n\nBrowsing by project node\n\uf0c1\n\n\n\n\nThe metadata submission portal \nhttps://data.gen3.org/\n can be used to browse an individual submission by node.   Just select a project and then click the \"browse nodes\" button to the right.    From there, you'll get a screen like below where you can query by node in the dropdown at the left.\n\n\n Example:  Browse by node \n\n\n\n\n\nYou can also use this feature to download the tsv associated with the node, or if you have \"write\" access to the this project, delete existing nodes.   \n\n\n\n\nGraphing a project\n\uf0c1\n\n\n\n\nYou can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.  \n\n\n Example:  Graphing a project", 
            "title": "Working with the API"
        }, 
        {
            "location": "/appendices/api-gen3/#working-with-the-api", 
            "text": "", 
            "title": "Working with the API"
        }, 
        {
            "location": "/appendices/api-gen3/#what-does-the-api-do", 
            "text": "The API is created programmatically based on the  Gen3 commons data model .   All of the work Gen3 data contributors do to prepare their metadata powers the API (see steps  4-6 in the Data Contribution section ).     With the API in place, users can submit queries to find whatever metadata information they might like across the Gen3 commons.   The API can be queried programmatically or through provided tools like the submission portal.    We use  GraphQL  to manage the metadata in the Gen3 commons.  To learn the basics of writing queries in Graph QL, we recommend  this introduction .", 
            "title": "What does the API do?"
        }, 
        {
            "location": "/appendices/api-gen3/#whats-an-example-of-the-api-at-work", 
            "text": "The Gen3 commons team has created a few matrices to help describe submitted data.   These are linked to from:  https://www.gen3.org/data-group/  These query the API for the desired metadata and return the matrices.   They run on cron jobs that update hourly, so if you're submitting data or adding to the commons, you can watch your entry appear.   If you are a member and would like to view these matrices, contact info@gen3.org for a username and password.", 
            "title": "What's an example of the API at work?"
        }, 
        {
            "location": "/appendices/api-gen3/#credentials-to-query-the-api", 
            "text": "Like your credentials that let you access \"raw\" data in the object store or your ssh keys that let you access a VM, there is also a credential that lets you programmatically query or submit data to the API. This credential is used every time you make an API call.  Each API key is valid for a month and is used to receive a temporary access token that is valid for only 30 minutes. The access token is what must be sent to the Gen3 API to access data in the commons.  For users granted data access, the API key is provided on your Profile page when you click \"Create API key\".     While displayed, you can click \"copy\" to copy the API key to your clipboard or \"download\" to download a \"credentials.json\" file containing the id/key pair in json format.     In python, the following command is sent, using the module \"requests\", to receive the access token:  # Save your copied credentials.json from the website into a variable  key :\nkey =  {  api_key :  actual-key ,  key_id :  a-key-uuid  }\n\n# Import the  requests  python module:\nimport requests\n\n# Pass the API key to the Gen3 API using  requests.post  to receive the access token:\ntoken = requests.post('https://gen3.commons.io/user/credentials/cdis/access_token', json=key).json()\n\n# Now you should see your access_token displayed when you enter:\ntoken  When submitting a graphQL query to the Gen3 API, or requesting data download/upload, include the access token in your request header:  headers = {'Authorization': 'bearer '+ token['access_token']}\n\n# a graphQL endpoint query using the  key  json:\nquery = {'query': {project(first:0){project_id id}} };\nql = requests.post('https://gen3.commons.io/api/v0/submission/graphql/', json=query, headers=headers)\nprint(ql.text) # display the response\n\n# Data download via API endpoint request:\ndurl = 'https://gen3.commons.io/api/v0/submission/ program / project /export?format=tsv ids=' + ids[0:-1] # define the download url with the UUIDs of the records to download in  ids  list\ndl = requests.get(durl, headers=headers)\nprint(dl.text) # display response\n\n# Data upload via API endpoint request:\nheaders['content-type']='text/tab-separated-values' # add the content-type to header\nu = requests.put('https://gen3.commons.io/api/v0/submission/project-id', data=tsv, headers=headers)  If you receive an error like \"You don't have access... \", then you will most likely need to update your API key or request a new access token.", 
            "title": "Credentials to query the API"
        }, 
        {
            "location": "/appendices/api-gen3/#queries-in-the-submission-portal-graphql", 
            "text": "You can run queries directly in the submission portal by clicking the \"Query\" magnifying glass or directly at:  https://data.gen3.org/graphql . Queries are essential as you begin analyses. The query portal has been optimized to autocomplete fields based on content, increase speed and responsiveness, and generally make it easier for Gen3 members to find what they need.", 
            "title": "Queries in the submission portal:   graphQL"
        }, 
        {
            "location": "/appendices/api-gen3/#pagination-and-offsets-default-first-10-entries", 
            "text": "Queries by defult return the first 10 entries. If you want more than that, you can specify it in the query call:  (first:1000)  In the case that too many results are returned, you may receive a timeout error. In that case, you may want to use  pagination . For example, if there are 2,550 records returned, and your graphiQL query is timing out with  (first:3000) , then break your query into multiple queries with offsets:  (first:1000, offset:0)      # this will return records 0-1000\n(first:1000, offset:1000)   # this will return records 1000-2000\n(first:1000, offset:2000)   # this will return records 2000-2,550  Updating the example template  details from experiment  sample query to call the first 1000, the call becomes:    {\n     query :  query Test {\n        experiment (first:1000, submitter_id:  INSERT submitter_id ) {  \n            experimental_intent\n            experimental_description\n            number_samples_per_experimental_group\n            type_of_sample\n            type_of_specimen\n        }\n    }  \n}", 
            "title": "Pagination and Offsets:   Default = first 10 entries"
        }, 
        {
            "location": "/appendices/api-gen3/#browsing-by-project-node", 
            "text": "The metadata submission portal  https://data.gen3.org/  can be used to browse an individual submission by node.   Just select a project and then click the \"browse nodes\" button to the right.    From there, you'll get a screen like below where you can query by node in the dropdown at the left.", 
            "title": "Browsing by project node"
        }, 
        {
            "location": "/appendices/api-gen3/#graphing-a-project", 
            "text": "You can also review a graph of an individual project, toggling between views of the completed nodes and the full graph.", 
            "title": "Graphing a project"
        }, 
        {
            "location": "/appendices/proxy-whitelist/", 
            "text": "Working with the proxy and whitelists\n\uf0c1\n\n\n\n\n Working with the Proxy \n\n\n\nTo prevent unauthorized traffic, the Gen3 VPC utilizes a proxy.   If you are using one of the custom VMs setup, there is already a line in your .bashrc file to handle traffic requests.   \n\n\nexport http_proxy=http://cloud-proxy.internal.io:3128\nexport https_proxy=$http_proxy\n\n\n\n\nAlternatively, if you have a different service or a tool that needs to call out, you can set the proxy with each command.  \n\n\nhttps_proxy=https://cloud-proxy.internal.io:3128 aws s3 ls s3://gen3-data/ --profile \nprofilename\n\n\n\n\n\n Whitelists \n\n\n\nAdditionally, to aid Gen3 Commons security, tool installation from outside sources is managed through a whitelist.   If you have problems installing a tool you need for your work, contact \n and with a list of any sites you might wish to install tools from.    After passing a security review,  these can be added to the whitelist to facilitate access.", 
            "title": "Working with the Proxy and Whitelist"
        }, 
        {
            "location": "/appendices/proxy-whitelist/#working-with-the-proxy-and-whitelists", 
            "text": "", 
            "title": "Working with the proxy and whitelists"
        }, 
        {
            "location": "/appendices/timepoints/", 
            "text": "Managing timepoints in a submission\n\uf0c1\n\n\n\n\nSome elements of submitted datasets could be related to each other in time.   To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates.   Every other date field is anchored by the \"index_date\" in the \"case\" node.  \n\n\nIn this field you can have things like \"Study Enrollment\" or \"Diagnosis\".   Study the case node in the dictionary for more information on the index_date field:  https://data.Gen3.org/dd/case\n\n\n Examples of submissions using multiple date times \n\n\n\nPatient A enrolls in a study on July 1, and has a sample taken on July 10. For patient A you would report:\n\n case.index_date = \"Study Enrollment\"\n\n biospecimen.days_to_procurement = July 10 - July 1 = 9\n\n\nAlternatively if they were diagnosed 2 years before the study began and you wanted to use that as the index_date nothing is stopping you:\n\n case.index_date = \"Diagnosis\"\n\n biospecimen.days_to_procurment = July 10, 2017 - July 1, 2015 =  739\n\n\n Negative Dates \n\n\n\nDays to values can also be negative. If you have an index_date event that occurs after the event, you would present those days_to values as negative. If Patient A had a biospecimen taken when they were initially diagnosed:\n\n case.index_date = \"Study Enrollment\"\n\n biospecimen.days_to_procurement = July 10, 2015 - July 1, 2017 = -721\n\n\n No Time Series \n\n\n\nThe days_to_procurement and days_to_collection are required fields. If you do not have any data for these, we allow escape values of \"Unknown\" and \"Not Applicable\". Please use \"Unknown\" in the instances where you have established a time series but are unable to pin down the date of the event. Use \"Not Applicable\" if you do not have a time series at all.", 
            "title": "Managing Timepoints"
        }, 
        {
            "location": "/appendices/timepoints/#managing-timepoints-in-a-submission", 
            "text": "Some elements of submitted datasets could be related to each other in time.   To stay in compliance with HIPAA, Gen3 commons create timelines without using real dates.   Every other date field is anchored by the \"index_date\" in the \"case\" node.    In this field you can have things like \"Study Enrollment\" or \"Diagnosis\".   Study the case node in the dictionary for more information on the index_date field:  https://data.Gen3.org/dd/case", 
            "title": "Managing timepoints in a submission"
        }, 
        {
            "location": "/appendices/template-tsvs/", 
            "text": "Template TSVs for metadata submission\n\uf0c1\n\n\n\n\nThis folder contains all potential entities that could be submitted to the Gen3 data model. Some of the entities are optional.\n\n\nYou can find a complete and detailed description of the new data dictionary \nhere\n.\n\n\nA project will need to be created and you need to be \ngranted submitter access\n before you submit everything under a project.  You will start your submission from \nstudy.tsv\n.\n\n\nstudy.tsv\n\n\ncase.tsv\n\n\nbiospecimen.tsv\n\n\nsample.tsv\n\n\naliquot.tsv\n\n\nanalyte.tsv", 
            "title": "Template TSVs"
        }, 
        {
            "location": "/appendices/template-tsvs/#template-tsvs-for-metadata-submission", 
            "text": "This folder contains all potential entities that could be submitted to the Gen3 data model. Some of the entities are optional.  You can find a complete and detailed description of the new data dictionary  here .  A project will need to be created and you need to be  granted submitter access  before you submit everything under a project.  You will start your submission from  study.tsv .  study.tsv  case.tsv  biospecimen.tsv  sample.tsv  aliquot.tsv  analyte.tsv", 
            "title": "Template TSVs for metadata submission"
        }
    ]
}